{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQ8_RRcaZNXV"
   },
   "source": [
    "## This assignment is designed for automated pathology detection for Medical Images in a relalistic setup, i.e. each image may have multiple pathologies/disorders. \n",
    "### The goal, for you as an MLE, is to design models and methods to predictively detect pathological images and explain the pathology sites in the image data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eK8M8sjWZVg9"
   },
   "source": [
    "## Data for this assignment is taken from a Kaggle contest: https://www.kaggle.com/c/vietai-advance-course-retinal-disease-detection/overview\n",
    "Explanation of the data set:\n",
    "The training data set contains 3435 retinal images that represent multiple pathological disorders. The patholgy classes and corresponding labels are: included in 'train.csv' file and each image can have more than one class category (multiple pathologies).\n",
    "The labels for each image are\n",
    "\n",
    "```\n",
    "-opacity (0), \n",
    "-diabetic retinopathy (1), \n",
    "-glaucoma (2),\n",
    "-macular edema (3),\n",
    "-macular degeneration (4),\n",
    "-retinal vascular occlusion (5)\n",
    "-normal (6)\n",
    "```\n",
    "The test data set contains 350 unlabelled images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4UyLYQX9JpD"
   },
   "source": [
    "# Label Propogation\n",
    "\n",
    "### Setting up Label Propogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhswtM6oAo--"
   },
   "source": [
    "## Task 3: Using the unlabelled data set in the 'test' folder augment the training data (semi-supervised learning) and report the variation in classification performance on test data set.(15 points)\n",
    "[You may use any method of your choice, one possible way is mentioned below.] \n",
    "\n",
    "```\n",
    "Hint: \n",
    "a. Train a model using the 'train' split.\n",
    "b. Pass the unlabelled images through the trained model and retrieve the dense layer feature prior to classification layer. Using this dense layer as representative of the image, apply label propagation to retrieve labels correspndng to the unbalelled data.\n",
    "c. Next, concatenate the train data with the unlabelled data (that has now been self labelled) and retrain the network.\n",
    "d. Report classification performance on test data\n",
    "Use the unlabelled test data  to improve classification performance by using a semi-supervised label-propagation/self-labelling approach. (20 points)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDtJ-ochsLL9"
   },
   "source": [
    "# Task 3: Label Prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bljPwe0Hac4s"
   },
   "outputs": [],
   "source": [
    "# importing the required modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image, ImageEnhance\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "#from google.colab.patches import cv2_imshow\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.backend import gradients, mean\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "O32oTKmbZJJu"
   },
   "outputs": [],
   "source": [
    "# find path for the data\n",
    "data_path = 'Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "rn4xxeBraC0r",
    "outputId": "71dab59f-13c7-4c5b-e713-7d9c0b7f3544"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>opacity</th>\n",
       "      <th>diabetic retinopathy</th>\n",
       "      <th>glaucoma</th>\n",
       "      <th>macular edema</th>\n",
       "      <th>macular degeneration</th>\n",
       "      <th>retinal vascular occlusion</th>\n",
       "      <th>normal</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filename</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>c24a1b14d253.jpg</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9ee905a41651.jpg</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3f58d128caf6.jpg</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4ce6599e7b20.jpg</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0def470360e4.jpg</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  opacity  diabetic retinopathy  glaucoma  macular edema  \\\n",
       "filename                                                                   \n",
       "c24a1b14d253.jpg        0                     0         0              0   \n",
       "9ee905a41651.jpg        0                     0         0              0   \n",
       "3f58d128caf6.jpg        0                     0         1              0   \n",
       "4ce6599e7b20.jpg        1                     0         0              0   \n",
       "0def470360e4.jpg        1                     0         0              0   \n",
       "\n",
       "                  macular degeneration  retinal vascular occlusion  normal  \n",
       "filename                                                                    \n",
       "c24a1b14d253.jpg                     0                           1       0  \n",
       "9ee905a41651.jpg                     0                           1       0  \n",
       "3f58d128caf6.jpg                     0                           0       0  \n",
       "4ce6599e7b20.jpg                     1                           0       0  \n",
       "0def470360e4.jpg                     1                           0       0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the training labels\n",
    "train_labels = pd.read_csv(os.path.join(data_path, 'train/train.csv'))\n",
    "train_labels.set_index('filename', inplace=True)\n",
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PljE5vNbXwd"
   },
   "source": [
    "### Create Data Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "FSvItXQUbPXa"
   },
   "outputs": [],
   "source": [
    "# function to load images to numpy array and normalize\n",
    "def img_to_np(img_path):\n",
    "    # open image using PIL\n",
    "    img_temp = Image.open(img_path)\n",
    "    img_temp = img_temp.resize((IMAGE_SIZE,IMAGE_SIZE), resample=1)\n",
    "\n",
    "    # convert to np array\n",
    "    img_temp = np.array(img_temp).astype(np.float32)\n",
    "    img_temp = img_temp/255.0\n",
    "    \n",
    "    return img_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "c2FOS_XA3Kgb"
   },
   "outputs": [],
   "source": [
    "# define image size and channels\n",
    "IMAGE_SIZE = 224\n",
    "CHANNELS = 3\n",
    "\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# direcotries containing files\n",
    "test_dir = 'Data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qNJoybPw4zML",
    "outputId": "1dd12c57-fa14-467e-d1fd-014c6363c8ff"
   },
   "outputs": [],
   "source": [
    "# load trained model\n",
    "model_pretrained = tf.keras.models.load_model('models/dbr_v1.h5')\n",
    "model_pretrained.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 222, 222, 256 7168        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 111, 111, 256 0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 109, 109, 256 590080      max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 54, 54, 256)  0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 52, 52, 64)   147520      max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling2D) (None, 26, 26, 64)   0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 26, 26, 128)  8320        max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 109, 109, 32) 73760       max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling2D) (None, 13, 13, 128)  0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2D)  (None, 54, 54, 32)   0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 11, 11, 64)   73792       max_pooling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 52, 52, 32)   9248        max_pooling2d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling2D) (None, 5, 5, 64)     0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling2D) (None, 26, 26, 32)   0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 1600)         0           max_pooling2d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 24, 24, 32)   9248        max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 256)          409856      flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling2D) (None, 12, 12, 32)   0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 128)          32896       dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 4608)         0           max_pooling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 128)          16512       dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 64)           294976      flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 64)           8256        dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 64)           4160        dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 64)           0           dense_12[0][0]                   \n",
      "                                                                 dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 2)            130         add_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 1,685,922\n",
      "Trainable params: 0\n",
      "Non-trainable params: 1,685,922\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_pretrained.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "gE85ex-AE-VA"
   },
   "outputs": [],
   "source": [
    "# get last dense layer\n",
    "last_dense_layer = 'dense_13'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6doiG7zDA5q5"
   },
   "outputs": [],
   "source": [
    "# setup to get the output from the last dense layer\n",
    "last_layer = model_pretrained.get_layer(last_dense_layer)\n",
    "last_conv_layer_model = tf.keras.Model(model_pretrained.inputs, last_layer.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sDqVpkuHn1c6",
    "outputId": "4d703dfc-31b0-403f-b19a-df106530ba3e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a train array for passing to knn\n",
    "pred_train = np.array([ 'name', 'p1', 'p2', 'labels', 'label_num'])\n",
    "pred_train = np.expand_dims(pred_train, axis=0)\n",
    "pred_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_original_dir = 'Data/train/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "bcE-pGiwCyWq"
   },
   "outputs": [],
   "source": [
    "# iterating over the image folder to collect predictions, names and labels\n",
    "for pic in os.listdir(train_original_dir):\n",
    "    # get image array\n",
    "    im_temp = img_to_np(os.path.join(train_original_dir, pic))\n",
    "    im_temp = np.expand_dims(im_temp, axis=0)\n",
    "\n",
    "    pred_temp = []\n",
    "\n",
    "    # get image label\n",
    "    max_val = 1\n",
    "    labels = [i for i in train_labels.loc[pic].values]\n",
    "\n",
    "    if labels[1] == 1:\n",
    "        pred_temp.append(pic)\n",
    "        for j in last_conv_layer_model.predict(im_temp)[0]:\n",
    "            pred_temp.append(j)\n",
    "        pred_temp.append('diabetic retinopathy')\n",
    "        pred_temp.append(1)\n",
    "    elif labels[2] == 1:\n",
    "        continue\n",
    "    #    pred_temp.append(pic)\n",
    "    #    for j in last_conv_layer_model.predict(im_temp)[0]:\n",
    "    #        pred_temp.append(j)\n",
    "    #    pred_temp.append('glaucoma')\n",
    "    #    pred_temp.append(2)\n",
    "    elif labels[6] == 1:\n",
    "        pred_temp.append(pic)\n",
    "        for j in last_conv_layer_model.predict(im_temp)[0]:\n",
    "            pred_temp.append(j)\n",
    "        pred_temp.append('normal')\n",
    "        pred_temp.append(2)\n",
    "    else:\n",
    "        continue\n",
    "        #pred_temp.append(pic)\n",
    "        #for j in last_conv_layer_model.predict(im_temp)[0]:\n",
    "        #    pred_temp.append(j)\n",
    "        #pred_temp.append('other')\n",
    "        #pred_temp.append(4)\n",
    "\n",
    "    pred_train = np.append(pred_train, np.expand_dims(np.array(pred_temp), axis=0), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Wt8uMZ7fxB7b"
   },
   "outputs": [],
   "source": [
    "# extract specific columns from np array\n",
    "# use X = data[:, [1, 9]]\n",
    "X_train = pred_train[1:,[1, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "XxKhqb6pxtbb"
   },
   "outputs": [],
   "source": [
    "# extract targets\n",
    "y_train = pred_train[1:, [4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "IzwTWqqLW1VZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "q43H5DMumQJl"
   },
   "outputs": [],
   "source": [
    "# create a label propagation model\n",
    "prop = KNN(n_neighbors=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5z0xg-rZmxMX",
    "outputId": "27538494-26a1-4e49-8ca3-2a7e90a75709"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "prop.fit(X_train.astype(np.float32), y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rs8IK0rFrAYQ",
    "outputId": "18555cba-5d95-4153-88ec-c781d6422bee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict labels for the test folder\n",
    "# creating a test array for passing to knn model\n",
    "pred_test = np.array(['name', 'labels', 'label_num'])\n",
    "pred_test = np.expand_dims(pred_test, axis=0)\n",
    "pred_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "l1Zjb8B3rfNY"
   },
   "outputs": [],
   "source": [
    "# create labels for the test images\n",
    "# iterating over the image folder to collect predictions, names and labels\n",
    "for pic in os.listdir(test_dir):\n",
    "    # get image array\n",
    "    im_temp = img_to_np(os.path.join(test_dir, pic))\n",
    "    im_temp = np.expand_dims(im_temp, axis=0)\n",
    "\n",
    "    pred_temp = []\n",
    "\n",
    "    # get image predited labels\n",
    "    label_predicted = prop.predict(np.expand_dims(\n",
    "        last_conv_layer_model.predict(im_temp)[0], axis=0))\n",
    "  \n",
    "    # append labels to the new array\n",
    "    if int(label_predicted[0]) == 1:\n",
    "        pred_temp.append(pic)\n",
    "        pred_temp.append('diabetic retinopathy')\n",
    "        pred_temp.append(1)\n",
    "\n",
    "    #elif int(label_predicted[0]) == 2:\n",
    "    #    pred_temp.append(pic)\n",
    "    #    pred_temp.append('galucoma')\n",
    "    #    pred_temp.append(2)\n",
    "\n",
    "    elif int(label_predicted[0]) == 2:\n",
    "        pred_temp.append(pic)\n",
    "        pred_temp.append('normal')\n",
    "        pred_temp.append(3)\n",
    "\n",
    "    else:\n",
    "        continue\n",
    "        #pred_temp.append(pic)\n",
    "        #pred_temp.append('other')\n",
    "        #pred_temp.append(4)\n",
    "\n",
    "    pred_test = np.append(pred_test, np.expand_dims(np.array(pred_temp), axis=0), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test[:, [2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_LDHBe_va0c"
   },
   "source": [
    "### Append the test data to train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test_ = pred_test[1:]\n",
    "pred_test_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "27J4yQuxvd18"
   },
   "outputs": [],
   "source": [
    "# create new folders with images\n",
    "new_image_dir = 'Data/separated_pics_aug'\n",
    "\n",
    "for row in pred_test_:\n",
    "    file_name = row[0]\n",
    "    label_num = int(row[2])\n",
    "\n",
    "    if label_num == 1:\n",
    "        shutil.copy(os.path.join(test_dir, file_name), \n",
    "                    os.path.join(new_image_dir, 'diabetic retinopathy/'+file_name))\n",
    "  \n",
    "    #elif label_num == 2:\n",
    "    #    shutil.copy(os.path.join(test_img_dir, file_name), \n",
    "    #                os.path.join(new_image_dir, 'glaucoma/'+file_name))\n",
    "    \n",
    "    elif label_num == 3:\n",
    "        shutil.copy(os.path.join(test_dir, file_name), \n",
    "                    os.path.join(new_image_dir, 'normal/'+file_name))\n",
    "  \n",
    "    #else:\n",
    "    #    shutil.copy(os.path.join(test_img_dir, file_name), \n",
    "    #                os.path.join(new_image_dir, 'other/'+file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDdUACUU_aOJ"
   },
   "source": [
    "## Train on Concatenated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "755\n",
      "1057\n",
      "578\n",
      "578\n",
      "525\n",
      "573\n",
      "749\n",
      "1099\n"
     ]
    }
   ],
   "source": [
    "# check if the data has been added\n",
    "print(len(os.listdir('Data/separated_pics_train/diabetic retinopathy')))\n",
    "print(len(os.listdir('Data/separated_pics_aug/diabetic retinopathy')))\n",
    "print(len(os.listdir('Data/separated_pics_train/glaucoma')))\n",
    "print(len(os.listdir('Data/separated_pics_aug/glaucoma')))\n",
    "print(len(os.listdir('Data/separated_pics_train/normal')))\n",
    "print(len(os.listdir('Data/separated_pics_aug/normal')))\n",
    "print(len(os.listdir('Data/separated_pics_train/other')))\n",
    "print(len(os.listdir('Data/separated_pics_aug/other')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "id": "c2FOS_XA3Kgb"
   },
   "outputs": [],
   "source": [
    "# define image size and channels\n",
    "IMAGE_SIZE = 256\n",
    "CHANNELS = 3\n",
    "\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "id": "3WoCkfqj6pNU"
   },
   "outputs": [],
   "source": [
    "# training image dir\n",
    "train_img_dir = os.path.join(data_path, 'separated_pics_aug')\n",
    "\n",
    "# testing image dir\n",
    "test_img_dir = 'Data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "id": "ivs-oNfSwdnW"
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=10.,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    channel_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    brightness_range=(-0.5,0.5),\n",
    "    fill_mode='constant',\n",
    "    data_format=\"channels_last\",\n",
    "    #preprocessing_function=tf.keras.applications.vgg19.preprocess_input,\n",
    "    preprocessing_function=contrast,\n",
    "    validation_split = 0.3,\n",
    "    rescale=1.0/255.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "id": "nsUJYt0mytWI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2072 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# create training data\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    directory=train_img_dir,\n",
    "    shuffle=True, \n",
    "    target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    subset='training'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 885 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# create training data\n",
    "val_gen = train_datagen.flow_from_directory(\n",
    "    directory=train_img_dir,\n",
    "    shuffle=True, \n",
    "    target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oR8iRNCIzpvy"
   },
   "source": [
    "### Creating callback for saving models and stopping training if certain accuracy is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ev1qOfkS0HYb"
   },
   "source": [
    "Download application from the tf.keras.application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "id": "3y2sgtyW0DCn"
   },
   "outputs": [],
   "source": [
    "# importing resnet50 from tensorflow applications\n",
    "base_model = tf.keras.applications.ResNet50(\n",
    "    input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "id": "_gcyfhHO01y0"
   },
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "#base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "id": "utefTjwS1dQf"
   },
   "outputs": [],
   "source": [
    "# select a layer to start optimizing from, \n",
    "# here we will train only the later half of the model\n",
    "train_layer = base_model.get_layer('conv4_block4_1_conv')\n",
    "\n",
    "# get train_layer index\n",
    "layer_index = base_model.layers.index(train_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "id": "kI5lDdNv2-nu"
   },
   "outputs": [],
   "source": [
    "# make layers before the training layer non trainable\n",
    "for layer in base_model.layers[:layer_index]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skipping the final activation layer and just getting the logits as these will be used for the final part of the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "id": "EnLvN2os25WE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Functional)        (None, 7, 7, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_16  (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 4)                 8196      \n",
      "=================================================================\n",
      "Total params: 23,595,908\n",
      "Trainable params: 18,340,356\n",
      "Non-trainable params: 5,255,552\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 1.3753 - accuracy: 0.3327INFO:tensorflow:Assets written to: models/acc_ckpts/assets\n",
      "32/32 [==============================] - 107s 3s/step - loss: 1.3753 - accuracy: 0.3327 - val_loss: 435.1318 - val_accuracy: 0.3678\n",
      "Epoch 2/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 1.3614 - accuracy: 0.3675INFO:tensorflow:Assets written to: models/acc_ckpts/assets\n",
      "32/32 [==============================] - 90s 3s/step - loss: 1.3614 - accuracy: 0.3675 - val_loss: 29.0607 - val_accuracy: 0.3798\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - 68s 2s/step - loss: 1.3663 - accuracy: 0.3635 - val_loss: 13.4626 - val_accuracy: 0.3738\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 1.3590 - accuracy: 0.3635INFO:tensorflow:Assets written to: models/acc_ckpts/assets\n",
      "32/32 [==============================] - 90s 3s/step - loss: 1.3590 - accuracy: 0.3635 - val_loss: 2.0717 - val_accuracy: 0.2464\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - 70s 2s/step - loss: 1.4009 - accuracy: 0.3277 - val_loss: 1.3582 - val_accuracy: 0.3702\n",
      "Epoch 6/100\n",
      "32/32 [==============================] - 69s 2s/step - loss: 1.3684 - accuracy: 0.3242 - val_loss: 1.8668 - val_accuracy: 0.3750\n",
      "Epoch 7/100\n",
      "32/32 [==============================] - 70s 2s/step - loss: 1.3624 - accuracy: 0.3561 - val_loss: 1.4865 - val_accuracy: 0.3810\n",
      "Epoch 8/100\n",
      "32/32 [==============================] - 68s 2s/step - loss: 1.3720 - accuracy: 0.3635 - val_loss: 1.4586 - val_accuracy: 0.3726\n",
      "Epoch 9/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 1.3560 - accuracy: 0.3591INFO:tensorflow:Assets written to: models/acc_ckpts/assets\n",
      "32/32 [==============================] - 97s 3s/step - loss: 1.3560 - accuracy: 0.3591 - val_loss: 1.3504 - val_accuracy: 0.3726\n",
      "Epoch 10/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 1.3482 - accuracy: 0.3611INFO:tensorflow:Assets written to: models/acc_ckpts/assets\n",
      "32/32 [==============================] - 95s 3s/step - loss: 1.3482 - accuracy: 0.3611 - val_loss: 1.3869 - val_accuracy: 0.3654\n",
      "Epoch 11/100\n",
      "32/32 [==============================] - 68s 2s/step - loss: 1.3536 - accuracy: 0.3621 - val_loss: 1.3504 - val_accuracy: 0.3702\n",
      "Epoch 12/100\n",
      "32/32 [==============================] - 66s 2s/step - loss: 1.3606 - accuracy: 0.3720 - val_loss: 1.3551 - val_accuracy: 0.3786\n",
      "Epoch 13/100\n",
      "32/32 [==============================] - 67s 2s/step - loss: 1.3632 - accuracy: 0.3586 - val_loss: 1.3409 - val_accuracy: 0.3750\n",
      "Epoch 14/100\n",
      "32/32 [==============================] - 70s 2s/step - loss: 1.3624 - accuracy: 0.3665 - val_loss: 1.3745 - val_accuracy: 0.3738\n",
      "Epoch 15/100\n",
      "32/32 [==============================] - 70s 2s/step - loss: 1.3513 - accuracy: 0.3625 - val_loss: 1.3861 - val_accuracy: 0.3714\n",
      "Epoch 16/100\n",
      "32/32 [==============================] - 69s 2s/step - loss: 1.3591 - accuracy: 0.3740 - val_loss: 1.3594 - val_accuracy: 0.3750\n",
      "Epoch 17/100\n",
      "32/32 [==============================] - 67s 2s/step - loss: 1.3562 - accuracy: 0.3721 - val_loss: 1.3487 - val_accuracy: 0.3690\n",
      "Epoch 18/100\n",
      "32/32 [==============================] - 67s 2s/step - loss: 1.3701 - accuracy: 0.3461 - val_loss: 1.3646 - val_accuracy: 0.3750\n",
      "Epoch 19/100\n",
      "32/32 [==============================] - 69s 2s/step - loss: 1.3548 - accuracy: 0.3710 - val_loss: 1.3737 - val_accuracy: 0.3678\n",
      "Epoch 20/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 1.3472 - accuracy: 0.3616INFO:tensorflow:Assets written to: models/acc_ckpts/assets\n",
      "32/32 [==============================] - 91s 3s/step - loss: 1.3472 - accuracy: 0.3616 - val_loss: 1.5106 - val_accuracy: 0.3666\n",
      "Epoch 21/100\n",
      "32/32 [==============================] - 66s 2s/step - loss: 1.3587 - accuracy: 0.3735 - val_loss: 1.3437 - val_accuracy: 0.3738\n",
      "Epoch 22/100\n",
      "32/32 [==============================] - 66s 2s/step - loss: 1.3572 - accuracy: 0.3491 - val_loss: 1.3631 - val_accuracy: 0.3750\n",
      "Epoch 23/100\n",
      "32/32 [==============================] - 65s 2s/step - loss: 1.3518 - accuracy: 0.3721 - val_loss: 1.4950 - val_accuracy: 0.3666\n",
      "Epoch 24/100\n",
      "32/32 [==============================] - 65s 2s/step - loss: 1.3560 - accuracy: 0.3685 - val_loss: 1.3742 - val_accuracy: 0.3762\n",
      "Epoch 25/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 1.3470 - accuracy: 0.3740INFO:tensorflow:Assets written to: models/acc_ckpts/assets\n",
      "32/32 [==============================] - 96s 3s/step - loss: 1.3470 - accuracy: 0.3740 - val_loss: 1.3728 - val_accuracy: 0.2548\n",
      "Epoch 26/100\n",
      "32/32 [==============================] - 63s 2s/step - loss: 1.3668 - accuracy: 0.3406 - val_loss: 1.3971 - val_accuracy: 0.1959\n",
      "Epoch 27/100\n",
      "32/32 [==============================] - 62s 2s/step - loss: 1.3609 - accuracy: 0.3670 - val_loss: 1.3876 - val_accuracy: 0.2560\n",
      "Epoch 28/100\n",
      "32/32 [==============================] - 63s 2s/step - loss: 1.3532 - accuracy: 0.3750 - val_loss: 1.3817 - val_accuracy: 0.2548\n",
      "Epoch 29/100\n",
      "32/32 [==============================] - 62s 2s/step - loss: 1.3514 - accuracy: 0.3531 - val_loss: 1.3839 - val_accuracy: 0.2512\n",
      "Epoch 30/100\n",
      "32/32 [==============================] - 66s 2s/step - loss: 1.3609 - accuracy: 0.3735 - val_loss: 1.5481 - val_accuracy: 0.3702\n",
      "Epoch 31/100\n",
      "32/32 [==============================] - 67s 2s/step - loss: 1.3631 - accuracy: 0.3616 - val_loss: 7.4031 - val_accuracy: 0.2548\n",
      "Epoch 32/100\n",
      "32/32 [==============================] - 69s 2s/step - loss: 1.3580 - accuracy: 0.3705 - val_loss: 3.4287 - val_accuracy: 0.2500\n",
      "Epoch 33/100\n",
      "32/32 [==============================] - 71s 2s/step - loss: 1.3611 - accuracy: 0.3591 - val_loss: 1.3431 - val_accuracy: 0.3738\n",
      "Epoch 34/100\n",
      "32/32 [==============================] - 71s 2s/step - loss: 1.3599 - accuracy: 0.3586 - val_loss: 1.3855 - val_accuracy: 0.2572\n",
      "Epoch 35/100\n",
      "32/32 [==============================] - 71s 2s/step - loss: 1.3603 - accuracy: 0.3638 - val_loss: 1.3781 - val_accuracy: 0.2608\n",
      "Epoch 36/100\n",
      "32/32 [==============================] - 69s 2s/step - loss: 1.3627 - accuracy: 0.3725 - val_loss: 1.3698 - val_accuracy: 0.3702\n",
      "Epoch 37/100\n",
      "32/32 [==============================] - 69s 2s/step - loss: 1.3653 - accuracy: 0.3710 - val_loss: 1.3524 - val_accuracy: 0.3714\n",
      "Epoch 38/100\n",
      "32/32 [==============================] - 65s 2s/step - loss: 1.3619 - accuracy: 0.3506 - val_loss: 1.3420 - val_accuracy: 0.3726\n",
      "Epoch 39/100\n",
      "32/32 [==============================] - 65s 2s/step - loss: 1.3640 - accuracy: 0.3386 - val_loss: 1.3431 - val_accuracy: 0.3762\n",
      "Epoch 40/100\n",
      "32/32 [==============================] - 64s 2s/step - loss: 1.3502 - accuracy: 0.3695 - val_loss: 1.3472 - val_accuracy: 0.3714\n",
      "Epoch 41/100\n",
      "32/32 [==============================] - 65s 2s/step - loss: 1.3640 - accuracy: 0.3715 - val_loss: 1.3476 - val_accuracy: 0.3762\n",
      "Epoch 42/100\n",
      "32/32 [==============================] - 65s 2s/step - loss: 1.3487 - accuracy: 0.3710 - val_loss: 1.4878 - val_accuracy: 0.3702\n",
      "Epoch 43/100\n",
      "32/32 [==============================] - 69s 2s/step - loss: 1.3659 - accuracy: 0.3511 - val_loss: 1.4192 - val_accuracy: 0.2500\n",
      "Epoch 44/100\n",
      "32/32 [==============================] - 69s 2s/step - loss: 1.3541 - accuracy: 0.3556 - val_loss: 1.3967 - val_accuracy: 0.3642\n",
      "Epoch 45/100\n",
      "32/32 [==============================] - 70s 2s/step - loss: 1.3495 - accuracy: 0.3720 - val_loss: 1.3650 - val_accuracy: 0.3714\n",
      "Epoch 46/100\n",
      "32/32 [==============================] - 70s 2s/step - loss: 1.3519 - accuracy: 0.3720 - val_loss: 1.3723 - val_accuracy: 0.3726\n",
      "Epoch 47/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 1.3465 - accuracy: 0.3601INFO:tensorflow:Assets written to: models/acc_ckpts/assets\n",
      "32/32 [==============================] - 90s 3s/step - loss: 1.3465 - accuracy: 0.3601 - val_loss: 1.3409 - val_accuracy: 0.3762\n",
      "Epoch 48/100\n",
      "32/32 [==============================] - 68s 2s/step - loss: 1.3521 - accuracy: 0.3720 - val_loss: 1.3455 - val_accuracy: 0.3726\n",
      "Epoch 49/100\n",
      "32/32 [==============================] - 67s 2s/step - loss: 1.3490 - accuracy: 0.3705 - val_loss: 1.3418 - val_accuracy: 0.3666\n",
      "Epoch 50/100\n",
      "32/32 [==============================] - 65s 2s/step - loss: 1.3504 - accuracy: 0.3720 - val_loss: 1.3773 - val_accuracy: 0.3702\n",
      "Epoch 51/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 1.3465 - accuracy: 0.3640INFO:tensorflow:Assets written to: models/acc_ckpts/assets\n",
      "32/32 [==============================] - 88s 3s/step - loss: 1.3465 - accuracy: 0.3640 - val_loss: 1.4250 - val_accuracy: 0.3702\n",
      "Epoch 52/100\n",
      "32/32 [==============================] - 64s 2s/step - loss: 1.3528 - accuracy: 0.3720 - val_loss: 1.3582 - val_accuracy: 0.3750\n",
      "Epoch 53/100\n",
      "32/32 [==============================] - 67s 2s/step - loss: 1.3516 - accuracy: 0.3740 - val_loss: 1.3453 - val_accuracy: 0.3762\n",
      "Epoch 54/100\n",
      "32/32 [==============================] - 66s 2s/step - loss: 1.3522 - accuracy: 0.3730 - val_loss: 1.3654 - val_accuracy: 0.3774\n",
      "Epoch 55/100\n",
      "32/32 [==============================] - 67s 2s/step - loss: 1.3497 - accuracy: 0.3720 - val_loss: 1.3726 - val_accuracy: 0.3702\n",
      "Epoch 56/100\n",
      "32/32 [==============================] - 67s 2s/step - loss: 1.3564 - accuracy: 0.3700 - val_loss: 1.3628 - val_accuracy: 0.3750\n",
      "Epoch 57/100\n",
      "32/32 [==============================] - 67s 2s/step - loss: 1.3491 - accuracy: 0.3521 - val_loss: 1.3489 - val_accuracy: 0.3666\n",
      "Epoch 58/100\n",
      "32/32 [==============================] - 67s 2s/step - loss: 1.3492 - accuracy: 0.3715 - val_loss: 1.3536 - val_accuracy: 0.3726\n",
      "Epoch 59/100\n",
      "32/32 [==============================] - 67s 2s/step - loss: 1.3554 - accuracy: 0.3705 - val_loss: 1.3715 - val_accuracy: 0.3786\n",
      "Epoch 60/100\n",
      "32/32 [==============================] - 67s 2s/step - loss: 1.3511 - accuracy: 0.3710 - val_loss: 1.3614 - val_accuracy: 0.3666\n",
      "Epoch 61/100\n",
      "32/32 [==============================] - 69s 2s/step - loss: 1.3487 - accuracy: 0.3720 - val_loss: 1.3471 - val_accuracy: 0.3702\n",
      "Epoch 62/100\n",
      "32/32 [==============================] - 69s 2s/step - loss: 1.3508 - accuracy: 0.3715 - val_loss: 1.3830 - val_accuracy: 0.2620\n",
      "Epoch 63/100\n",
      "32/32 [==============================] - 67s 2s/step - loss: 1.3573 - accuracy: 0.3526 - val_loss: 1.6262 - val_accuracy: 0.2584\n",
      "Epoch 64/100\n",
      "32/32 [==============================] - 67s 2s/step - loss: 1.3497 - accuracy: 0.3740 - val_loss: 1.3738 - val_accuracy: 0.3750\n",
      "Epoch 65/100\n",
      "32/32 [==============================] - 67s 2s/step - loss: 1.3509 - accuracy: 0.3730 - val_loss: 1.3449 - val_accuracy: 0.3786\n",
      "Epoch 66/100\n",
      "32/32 [==============================] - 68s 2s/step - loss: 1.3498 - accuracy: 0.3705 - val_loss: 1.3803 - val_accuracy: 0.2560\n",
      "Epoch 67/100\n",
      "32/32 [==============================] - 68s 2s/step - loss: 1.3523 - accuracy: 0.3705 - val_loss: 1.3608 - val_accuracy: 0.3666\n",
      "Epoch 68/100\n",
      "32/32 [==============================] - 71s 2s/step - loss: 1.3527 - accuracy: 0.3571 - val_loss: 1.3699 - val_accuracy: 0.3654\n",
      "Epoch 69/100\n",
      "32/32 [==============================] - 70s 2s/step - loss: 1.3481 - accuracy: 0.3690 - val_loss: 1.3783 - val_accuracy: 0.2632\n",
      "Epoch 70/100\n",
      "32/32 [==============================] - 68s 2s/step - loss: 1.3477 - accuracy: 0.3690 - val_loss: 1.3534 - val_accuracy: 0.3774\n",
      "Epoch 71/100\n",
      "32/32 [==============================] - 63s 2s/step - loss: 1.3509 - accuracy: 0.3735 - val_loss: 1.3737 - val_accuracy: 0.3726\n",
      "Epoch 72/100\n",
      "32/32 [==============================] - 63s 2s/step - loss: 1.3516 - accuracy: 0.3665 - val_loss: 1.3489 - val_accuracy: 0.3666\n",
      "Epoch 73/100\n",
      "32/32 [==============================] - 63s 2s/step - loss: 1.3487 - accuracy: 0.3710 - val_loss: 1.3655 - val_accuracy: 0.3690\n",
      "Epoch 74/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 1.3465 - accuracy: 0.3700INFO:tensorflow:Assets written to: models/acc_ckpts/assets\n",
      "32/32 [==============================] - 99s 3s/step - loss: 1.3465 - accuracy: 0.3700 - val_loss: 1.3638 - val_accuracy: 0.3678\n",
      "Epoch 75/100\n",
      "32/32 [==============================] - 70s 2s/step - loss: 1.3511 - accuracy: 0.3735 - val_loss: 1.3682 - val_accuracy: 0.3714\n",
      "Epoch 76/100\n",
      "32/32 [==============================] - 70s 2s/step - loss: 1.3527 - accuracy: 0.3720 - val_loss: 3.4073 - val_accuracy: 0.1971\n",
      "Epoch 77/100\n",
      "32/32 [==============================] - 69s 2s/step - loss: 1.3499 - accuracy: 0.3710 - val_loss: 1.5853 - val_accuracy: 0.3738\n",
      "Epoch 78/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 1.3464 - accuracy: 0.3740INFO:tensorflow:Assets written to: models/acc_ckpts/assets\n",
      "32/32 [==============================] - 95s 3s/step - loss: 1.3464 - accuracy: 0.3740 - val_loss: 1.3596 - val_accuracy: 0.3702\n",
      "Epoch 79/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 1.3456 - accuracy: 0.3720INFO:tensorflow:Assets written to: models/acc_ckpts/assets\n",
      "32/32 [==============================] - 90s 3s/step - loss: 1.3456 - accuracy: 0.3720 - val_loss: 1.3555 - val_accuracy: 0.3690\n",
      "Epoch 80/100\n",
      "32/32 [==============================] - 65s 2s/step - loss: 1.3491 - accuracy: 0.3740 - val_loss: 1.3459 - val_accuracy: 0.3738\n",
      "Epoch 81/100\n",
      "32/32 [==============================] - 64s 2s/step - loss: 1.3502 - accuracy: 0.3730 - val_loss: 1.3696 - val_accuracy: 0.3678\n",
      "Epoch 82/100\n",
      "32/32 [==============================] - 63s 2s/step - loss: 1.3500 - accuracy: 0.3700 - val_loss: 1.3601 - val_accuracy: 0.3726\n",
      "Epoch 83/100\n",
      "32/32 [==============================] - 69s 2s/step - loss: 1.3494 - accuracy: 0.3730 - val_loss: 1.3483 - val_accuracy: 0.3702\n",
      "Epoch 84/100\n",
      "32/32 [==============================] - 70s 2s/step - loss: 1.3474 - accuracy: 0.3735 - val_loss: 1.3497 - val_accuracy: 0.3738\n",
      "Epoch 85/100\n",
      "32/32 [==============================] - 72s 2s/step - loss: 1.3490 - accuracy: 0.3680 - val_loss: 1.3791 - val_accuracy: 0.2596\n",
      "Epoch 86/100\n",
      "32/32 [==============================] - 72s 2s/step - loss: 1.3511 - accuracy: 0.3680 - val_loss: 1.3473 - val_accuracy: 0.3798\n",
      "Epoch 87/100\n",
      "32/32 [==============================] - 70s 2s/step - loss: 1.3502 - accuracy: 0.3715 - val_loss: 1.3829 - val_accuracy: 0.2596\n",
      "Epoch 88/100\n",
      "32/32 [==============================] - 67s 2s/step - loss: 1.3470 - accuracy: 0.3715 - val_loss: 1.3804 - val_accuracy: 0.3762\n",
      "Epoch 89/100\n",
      "32/32 [==============================] - 67s 2s/step - loss: 1.3462 - accuracy: 0.3730 - val_loss: 1.3632 - val_accuracy: 0.3702\n",
      "Epoch 90/100\n",
      "32/32 [==============================] - 68s 2s/step - loss: 1.3486 - accuracy: 0.3690 - val_loss: 1.3410 - val_accuracy: 0.3738\n",
      "Epoch 91/100\n",
      "32/32 [==============================] - 66s 2s/step - loss: 1.3501 - accuracy: 0.3695 - val_loss: 1.3503 - val_accuracy: 0.3738\n",
      "Epoch 92/100\n",
      "32/32 [==============================] - 65s 2s/step - loss: 1.3465 - accuracy: 0.3705 - val_loss: 1.3487 - val_accuracy: 0.3702\n",
      "Epoch 93/100\n",
      "32/32 [==============================] - 66s 2s/step - loss: 1.3473 - accuracy: 0.3715 - val_loss: 1.3489 - val_accuracy: 0.3702\n",
      "Epoch 94/100\n",
      "32/32 [==============================] - 66s 2s/step - loss: 1.3493 - accuracy: 0.3690 - val_loss: 1.3411 - val_accuracy: 0.3750\n",
      "Epoch 95/100\n",
      "32/32 [==============================] - 70s 2s/step - loss: 1.3492 - accuracy: 0.3700 - val_loss: 1.3502 - val_accuracy: 0.3666\n",
      "Epoch 96/100\n",
      "32/32 [==============================] - 70s 2s/step - loss: 1.3488 - accuracy: 0.3730 - val_loss: 1.3847 - val_accuracy: 0.2524\n",
      "Epoch 97/100\n",
      "32/32 [==============================] - 69s 2s/step - loss: 1.3458 - accuracy: 0.3715 - val_loss: 1.3790 - val_accuracy: 0.2560\n",
      "Epoch 98/100\n",
      "32/32 [==============================] - 69s 2s/step - loss: 1.3469 - accuracy: 0.3730 - val_loss: 1.5980 - val_accuracy: 0.3822\n",
      "Epoch 99/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 1.3440 - accuracy: 0.3745"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "history = model.fit(train_gen,\n",
    "                    steps_per_epoch=train_gen.samples // BATCH_SIZE,\n",
    "                    epochs= 100,\n",
    "                    verbose=1,\n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_gen.samples // BATCH_SIZE,\n",
    "                    callbacks=[accuracy_callback])\n",
    "                    #callbacks=[stop_train, accuracy_callback, lr_decay])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plottig history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([1, 0])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/resnet50_augumented.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Rana ML_for_Computer_Vision.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
