{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQ8_RRcaZNXV"
   },
   "source": [
    "## This assignment is designed for automated pathology detection for Medical Images in a relalistic setup, i.e. each image may have multiple pathologies/disorders. \n",
    "### The goal, for you as an MLE, is to design models and methods to predictively detect pathological images and explain the pathology sites in the image data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eK8M8sjWZVg9"
   },
   "source": [
    "## Data for this assignment is taken from a Kaggle contest: https://www.kaggle.com/c/vietai-advance-course-retinal-disease-detection/overview\n",
    "Explanation of the data set:\n",
    "The training data set contains 3435 retinal images that represent multiple pathological disorders. The patholgy classes and corresponding labels are: included in 'train.csv' file and each image can have more than one class category (multiple pathologies).\n",
    "The labels for each image are\n",
    "\n",
    "```\n",
    "-opacity (0), \n",
    "-diabetic retinopathy (1), \n",
    "-glaucoma (2),\n",
    "-macular edema (3),\n",
    "-macular degeneration (4),\n",
    "-retinal vascular occlusion (5)\n",
    "-normal (6)\n",
    "```\n",
    "The test data set contains 350 unlabelled images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhDeT7B2Zk2S"
   },
   "source": [
    "# For this assignment, you are working with specialists for Diabetic Retinopathy and Glaucoma only, and your client is interested in a predictive learning model along with feature explanability and self-learning for Diabetic Retinopathy and Glaucoma vs. Normal images.\n",
    "# Design models and methods for the following tasks. Each task should be accompanied by code, plots/images (if applicable), tables (if applicable) and text:\n",
    "## Task 1: Build a classification model for Diabetic Retinopathy and Glaucoma vs normal images. You may consider multi-class classification vs. all-vs-one classification. Clearly state your choice and share details of your model, paremeters and hyper-paramaterization pprocess. (60 points)\n",
    "```\n",
    "a. Perform 70/30 data split and report performance scores on the test data set.\n",
    "b. You can choose to apply any data augmentation strategy. \n",
    "Explain your methods and rationale behind parameter selection.\n",
    "c. Show Training-validation curves to ensure overfitting and underfitting is avoided.\n",
    "```\n",
    "## Task 2: Visualize the heatmap/saliency/features using any method of your choice to demonstrate what regions of interest contribute to Diabetic Retinopathy and Glaucoma, respectively. (25 points)\n",
    "```\n",
    "Submit images/folder of images with heatmaps/features aligned on top of the images, or corresponding bounding boxes, and report what regions of interest in your opinion represent the pathological sites.\n",
    "```\n",
    "\n",
    "## Task 3: Using the unlabelled data set in the 'test' folder augment the training data (semi-supervised learning) and report the variation in classification performance on test data set.(15 points)\n",
    "[You may use any method of your choice, one possible way is mentioned below.] \n",
    "\n",
    "```\n",
    "Hint: \n",
    "a. Train a model using the 'train' split.\n",
    "b. Pass the unlabelled images through the trained model and retrieve the dense layer feature prior to classification layer. Using this dense layer as representative of the image, apply label propagation to retrieve labels correspndng to the unbalelled data.\n",
    "c. Next, concatenate the train data with the unlabelled data (that has now been self labelled) and retrain the network.\n",
    "d. Report classification performance on test data\n",
    "Use the unlabelled test data  to improve classification performance by using a semi-supervised label-propagation/self-labelling approach. (20 points)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDtJ-ochsLL9"
   },
   "source": [
    "# Task 1: Create a classifier\n",
    "\n",
    "The first attempt for this task was based on a multi-label strategy. I have kept some of the code for that in the notebook. However, there were several issues with that approach. The biggest one being the system not being able to form a very good decision boundary as a result everything started getting classified into a general category.\n",
    "\n",
    "The current approach is based on a conventional multiclass classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "id": "bljPwe0Hac4s"
   },
   "outputs": [],
   "source": [
    "# importing the required modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image, ImageEnhance\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "#from google.colab.patches import cv2_imshow\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.backend import gradients, mean\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "O32oTKmbZJJu"
   },
   "outputs": [],
   "source": [
    "# find path for the data\n",
    "data_path = 'Data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ox7d4ymeaUhY"
   },
   "source": [
    "### Process the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "rn4xxeBraC0r",
    "outputId": "71dab59f-13c7-4c5b-e713-7d9c0b7f3544"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>opacity</th>\n",
       "      <th>diabetic retinopathy</th>\n",
       "      <th>glaucoma</th>\n",
       "      <th>macular edema</th>\n",
       "      <th>macular degeneration</th>\n",
       "      <th>retinal vascular occlusion</th>\n",
       "      <th>normal</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filename</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>c24a1b14d253.jpg</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9ee905a41651.jpg</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3f58d128caf6.jpg</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4ce6599e7b20.jpg</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0def470360e4.jpg</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  opacity  diabetic retinopathy  glaucoma  macular edema  \\\n",
       "filename                                                                   \n",
       "c24a1b14d253.jpg        0                     0         0              0   \n",
       "9ee905a41651.jpg        0                     0         0              0   \n",
       "3f58d128caf6.jpg        0                     0         1              0   \n",
       "4ce6599e7b20.jpg        1                     0         0              0   \n",
       "0def470360e4.jpg        1                     0         0              0   \n",
       "\n",
       "                  macular degeneration  retinal vascular occlusion  normal  \n",
       "filename                                                                    \n",
       "c24a1b14d253.jpg                     0                           1       0  \n",
       "9ee905a41651.jpg                     0                           1       0  \n",
       "3f58d128caf6.jpg                     0                           0       0  \n",
       "4ce6599e7b20.jpg                     1                           0       0  \n",
       "0def470360e4.jpg                     1                           0       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the training labels\n",
    "train_labels = pd.read_csv(os.path.join(data_path, 'train/train.csv'))\n",
    "train_labels.set_index('filename', inplace=True)\n",
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61CUsMgi_K9d",
    "outputId": "1b7f088c-c93d-4d2e-b365-439d2e67eade"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diabetic retinopathy\n",
      "0    2680\n",
      "1     755\n",
      "Name: diabetic retinopathy, dtype: int64 \n",
      "\n",
      "glaucoma\n",
      "0    2838\n",
      "1     597\n",
      "Name: glaucoma, dtype: int64 \n",
      "\n",
      "macular edema\n",
      "0    2919\n",
      "1     516\n",
      "Name: macular edema, dtype: int64 \n",
      "\n",
      "macular degeneration\n",
      "0    2861\n",
      "1     574\n",
      "Name: macular degeneration, dtype: int64 \n",
      "\n",
      "retinal vascular occlusion\n",
      "0    2995\n",
      "1     440\n",
      "Name: retinal vascular occlusion, dtype: int64 \n",
      "\n",
      "normal\n",
      "0    2910\n",
      "1     525\n",
      "Name: normal, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check distribution of different disese and also overlap between different disease\n",
    "for label in train_labels.columns[1:]:\n",
    "    print(label)\n",
    "    print(train_labels[label].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PljE5vNbXwd"
   },
   "source": [
    "### Create Data Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "nLQowV-Vs-Mv"
   },
   "outputs": [],
   "source": [
    "# separating files into 4 categories\n",
    "\n",
    "## create separate dataset from existing training file\n",
    "original_folder = os.path.join(data_path, 'train/train')\n",
    "new_folder = os.path.join(data_path, 'separated_pics_train')\n",
    "\n",
    "# index = 1\n",
    "dbr_folder = os.path.join(new_folder, 'diabetic retinopathy')\n",
    "\n",
    "# index = 2\n",
    "glc_folder = os.path.join(new_folder, 'glaucoma')\n",
    "\n",
    "# index else\n",
    "other_folder = os.path.join(new_folder, 'other')\n",
    "\n",
    "# index = 6\n",
    "normal_folder = os.path.join(new_folder, 'normal')\n",
    "\n",
    "for i in range(len(train_labels)):\n",
    "    max_val = train_labels.iloc[i].max()\n",
    "    indices = [i for i, j in enumerate(train_labels.iloc[i]) if j == max_val]\n",
    "\n",
    "    '''commands to move files to designated folders'''\n",
    "    if 1 in indices:\n",
    "        shutil.copy(os.path.join(original_folder, train_labels.index[i]), \n",
    "                    os.path.join(dbr_folder, train_labels.index[i]))\n",
    "    \n",
    "    elif 2 in indices:\n",
    "        shutil.copy(os.path.join(original_folder, train_labels.index[i]), \n",
    "                    os.path.join(glc_folder, train_labels.index[i]))\n",
    "        \n",
    "    elif 6 in indices:\n",
    "        shutil.copy(os.path.join(original_folder, train_labels.index[i]), \n",
    "                    os.path.join(normal_folder, train_labels.index[i]))\n",
    "        \n",
    "    else:\n",
    "        shutil.copy(os.path.join(original_folder, train_labels.index[i]), \n",
    "                    os.path.join(other_folder, train_labels.index[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "id": "c2FOS_XA3Kgb"
   },
   "outputs": [],
   "source": [
    "# define image size and channels\n",
    "IMAGE_SIZE = 224\n",
    "CHANNELS = 3\n",
    "\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "id": "3WoCkfqj6pNU"
   },
   "outputs": [],
   "source": [
    "# training image dir\n",
    "train_img_dir = os.path.join(data_path, 'separated_pics_train')\n",
    "\n",
    "# testing image dir\n",
    "test_img_dir = 'Data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing function to highlight the green channel\n",
    "def green_channel(img):\n",
    "    img_out = img[:,:,1]\n",
    "    return img_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increasing contrast\n",
    "def contrast(img_np):\n",
    "    return(tf.image.adjust_contrast(img_np, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "id": "ivs-oNfSwdnW"
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=10.,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    channel_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    brightness_range=(-0.5,0.5),\n",
    "    fill_mode='constant',\n",
    "    data_format=\"channels_last\",\n",
    "    #preprocessing_function=tf.keras.applications.vgg19.preprocess_input,\n",
    "    preprocessing_function=contrast,\n",
    "    validation_split = 0.3,\n",
    "    rescale=1.0/255.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "id": "nsUJYt0mytWI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1827 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# create training data\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    directory=train_img_dir,\n",
    "    shuffle=True, \n",
    "    target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    subset='training'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 780 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# create training data\n",
    "val_gen = train_datagen.flow_from_directory(\n",
    "    directory=train_img_dir,\n",
    "    shuffle=True, \n",
    "    target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oR8iRNCIzpvy"
   },
   "source": [
    "### Creating callback for saving models and stopping training if certain accuracy is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "HA_7g6MF_K9j"
   },
   "outputs": [],
   "source": [
    "# define callback to stop training if a certain accuracy is achieved\n",
    "class mycallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(seld, epoch, logs={}):\n",
    "        if (logs.get('accuracy')>0.95):\n",
    "            print('\\nReached 95% accuracy so cancelling training')\n",
    "            self.model.stop_training = True\n",
    "    \n",
    "stop_train = mycallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "id": "64VmMELl_K9j"
   },
   "outputs": [],
   "source": [
    "# callback for saving checkpoints during training\n",
    "accuracy_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'models/acc_ckpts',\n",
    "    monitor='loss',\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode='auto',\n",
    "    save_freq='epoch',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for lr decay\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch % 20 != 0:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate decay\n",
    "lr_decay = tf.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ev1qOfkS0HYb"
   },
   "source": [
    "## Download application from the tf.keras.application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "id": "3y2sgtyW0DCn"
   },
   "outputs": [],
   "source": [
    "# importing ResNet50 from tensorflow applications\n",
    "base_model = tf.keras.applications.ResNet50(\n",
    "    input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "id": "_gcyfhHO01y0"
   },
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "#base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "id": "utefTjwS1dQf"
   },
   "outputs": [],
   "source": [
    "# select a layer to start optimizing from, \n",
    "# here we will train only the later half of the model\n",
    "train_layer = base_model.get_layer('conv4_block4_1_conv')\n",
    "\n",
    "# get train_layer index\n",
    "layer_index = base_model.layers.index(train_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "id": "kI5lDdNv2-nu"
   },
   "outputs": [],
   "source": [
    "# make layers before the training layer non trainable\n",
    "for layer in base_model.layers[:layer_index]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skipping the final activation layer and just getting the logits as these will be used for the final part of the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "id": "EnLvN2os25WE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Functional)        (None, 7, 7, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_15  (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 24,638,852\n",
      "Trainable params: 19,383,300\n",
      "Non-trainable params: 5,255,552\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(4)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "28/28 [==============================] - 53s 2s/step - loss: 9.4444 - accuracy: 0.2150 - val_loss: 7.8282 - val_accuracy: 0.2839\n",
      "Epoch 2/100\n",
      "28/28 [==============================] - 51s 2s/step - loss: 8.2100 - accuracy: 0.2791 - val_loss: 8.2060 - val_accuracy: 0.2878\n",
      "Epoch 3/100\n",
      "28/28 [==============================] - 50s 2s/step - loss: 8.1642 - accuracy: 0.2864 - val_loss: 8.3529 - val_accuracy: 0.2839\n",
      "Epoch 4/100\n",
      "28/28 [==============================] - 49s 2s/step - loss: 8.2373 - accuracy: 0.2887 - val_loss: 8.2479 - val_accuracy: 0.2839\n",
      "Epoch 5/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 8.1186 - accuracy: 0.2864 - val_loss: 8.2060 - val_accuracy: 0.2878\n",
      "Epoch 6/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 7.9996 - accuracy: 0.2859 - val_loss: 8.2479 - val_accuracy: 0.2878\n",
      "Epoch 7/100\n",
      "28/28 [==============================] - 49s 2s/step - loss: 8.2739 - accuracy: 0.2853 - val_loss: 8.2479 - val_accuracy: 0.2865\n",
      "Epoch 8/100\n",
      "28/28 [==============================] - 49s 2s/step - loss: 7.9448 - accuracy: 0.2842 - val_loss: 8.1850 - val_accuracy: 0.2891\n",
      "Epoch 9/100\n",
      "28/28 [==============================] - 46s 2s/step - loss: 8.3836 - accuracy: 0.2864 - val_loss: 8.2479 - val_accuracy: 0.2865\n",
      "Epoch 10/100\n",
      "28/28 [==============================] - 45s 2s/step - loss: 8.1916 - accuracy: 0.2898 - val_loss: 8.1850 - val_accuracy: 0.2904\n",
      "Epoch 11/100\n",
      "28/28 [==============================] - 45s 2s/step - loss: 8.4202 - accuracy: 0.2893 - val_loss: 8.2060 - val_accuracy: 0.2878\n",
      "Epoch 12/100\n",
      "28/28 [==============================] - 44s 2s/step - loss: 8.1368 - accuracy: 0.2847 - val_loss: 8.2899 - val_accuracy: 0.2826\n",
      "Epoch 13/100\n",
      "28/28 [==============================] - 45s 2s/step - loss: 8.1093 - accuracy: 0.2910 - val_loss: 8.2479 - val_accuracy: 0.2865\n",
      "Epoch 14/100\n",
      "28/28 [==============================] - 45s 2s/step - loss: 8.3196 - accuracy: 0.2864 - val_loss: 8.2479 - val_accuracy: 0.2839\n",
      "Epoch 15/100\n",
      "28/28 [==============================] - 45s 2s/step - loss: 8.1916 - accuracy: 0.2898 - val_loss: 8.2689 - val_accuracy: 0.2852\n",
      "Epoch 16/100\n",
      "28/28 [==============================] - 44s 2s/step - loss: 8.2008 - accuracy: 0.2796 - val_loss: 8.2060 - val_accuracy: 0.2878\n",
      "Epoch 17/100\n",
      "28/28 [==============================] - 45s 2s/step - loss: 7.9448 - accuracy: 0.2898 - val_loss: 8.2479 - val_accuracy: 0.2852\n",
      "Epoch 18/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 8.0636 - accuracy: 0.2813 - val_loss: 8.2269 - val_accuracy: 0.2891\n",
      "Epoch 19/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 7.9905 - accuracy: 0.2859 - val_loss: 8.2269 - val_accuracy: 0.2891\n",
      "Epoch 20/100\n",
      "28/28 [==============================] - 45s 2s/step - loss: 8.1916 - accuracy: 0.2881 - val_loss: 8.2060 - val_accuracy: 0.2891\n",
      "Epoch 21/100\n",
      "28/28 [==============================] - 44s 2s/step - loss: 8.2739 - accuracy: 0.2876 - val_loss: 8.2479 - val_accuracy: 0.2904\n",
      "Epoch 22/100\n",
      "28/28 [==============================] - 45s 2s/step - loss: 8.0728 - accuracy: 0.2864 - val_loss: 8.2689 - val_accuracy: 0.2852\n",
      "Epoch 23/100\n",
      "28/28 [==============================] - 45s 2s/step - loss: 8.4750 - accuracy: 0.2870 - val_loss: 8.2689 - val_accuracy: 0.2878\n",
      "Epoch 24/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 8.1002 - accuracy: 0.2825 - val_loss: 8.2479 - val_accuracy: 0.2865\n",
      "Epoch 25/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 8.2465 - accuracy: 0.2802 - val_loss: 8.2689 - val_accuracy: 0.2891\n",
      "Epoch 26/100\n",
      "28/28 [==============================] - 50s 2s/step - loss: 7.7985 - accuracy: 0.2847 - val_loss: 8.2060 - val_accuracy: 0.2891\n",
      "Epoch 27/100\n",
      "28/28 [==============================] - 50s 2s/step - loss: 8.0910 - accuracy: 0.2791 - val_loss: 8.2479 - val_accuracy: 0.2865\n",
      "Epoch 28/100\n",
      "28/28 [==============================] - 51s 2s/step - loss: 7.9539 - accuracy: 0.2836 - val_loss: 8.2269 - val_accuracy: 0.2865\n",
      "Epoch 29/100\n",
      "28/28 [==============================] - 51s 2s/step - loss: 7.9996 - accuracy: 0.2870 - val_loss: 8.2689 - val_accuracy: 0.2878\n",
      "Epoch 30/100\n",
      "28/28 [==============================] - 51s 2s/step - loss: 8.0088 - accuracy: 0.2870 - val_loss: 8.2479 - val_accuracy: 0.2852\n",
      "Epoch 31/100\n",
      "28/28 [==============================] - 50s 2s/step - loss: 7.7802 - accuracy: 0.2870 - val_loss: 8.2689 - val_accuracy: 0.2891\n",
      "Epoch 32/100\n",
      "28/28 [==============================] - 49s 2s/step - loss: 8.1276 - accuracy: 0.2830 - val_loss: 8.2269 - val_accuracy: 0.2878\n",
      "Epoch 33/100\n",
      "28/28 [==============================] - 50s 2s/step - loss: 8.0728 - accuracy: 0.2830 - val_loss: 8.1850 - val_accuracy: 0.2891\n",
      "Epoch 34/100\n",
      "28/28 [==============================] - 50s 2s/step - loss: 8.2190 - accuracy: 0.2836 - val_loss: 8.2689 - val_accuracy: 0.2865\n",
      "Epoch 35/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 7.9996 - accuracy: 0.2881 - val_loss: 8.2269 - val_accuracy: 0.2878\n",
      "Epoch 36/100\n",
      "28/28 [==============================] - 50s 2s/step - loss: 8.3196 - accuracy: 0.2796 - val_loss: 8.2479 - val_accuracy: 0.2852\n",
      "Epoch 37/100\n",
      "28/28 [==============================] - 49s 2s/step - loss: 8.2373 - accuracy: 0.2819 - val_loss: 8.1640 - val_accuracy: 0.2891\n",
      "Epoch 38/100\n",
      "28/28 [==============================] - 49s 2s/step - loss: 8.2648 - accuracy: 0.2819 - val_loss: 8.2689 - val_accuracy: 0.2852\n",
      "Epoch 39/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 8.2830 - accuracy: 0.2847 - val_loss: 8.2689 - val_accuracy: 0.2878\n",
      "Epoch 40/100\n",
      "28/28 [==============================] - 46s 2s/step - loss: 8.0819 - accuracy: 0.2881 - val_loss: 8.2689 - val_accuracy: 0.2878\n",
      "Epoch 41/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 8.1130 - accuracy: 0.2812 - val_loss: 8.2689 - val_accuracy: 0.2839\n",
      "Epoch 42/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 7.9996 - accuracy: 0.2819 - val_loss: 8.2269 - val_accuracy: 0.2878\n",
      "Epoch 43/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 8.1093 - accuracy: 0.2853 - val_loss: 8.2899 - val_accuracy: 0.2865\n",
      "Epoch 44/100\n",
      "28/28 [==============================] - 49s 2s/step - loss: 8.2556 - accuracy: 0.2791 - val_loss: 8.2269 - val_accuracy: 0.2865\n",
      "Epoch 45/100\n",
      "28/28 [==============================] - 51s 2s/step - loss: 8.2282 - accuracy: 0.2853 - val_loss: 8.2689 - val_accuracy: 0.2865\n",
      "Epoch 46/100\n",
      "28/28 [==============================] - 49s 2s/step - loss: 8.2922 - accuracy: 0.2830 - val_loss: 8.2479 - val_accuracy: 0.2839\n",
      "Epoch 47/100\n",
      "28/28 [==============================] - 50s 2s/step - loss: 7.9813 - accuracy: 0.2836 - val_loss: 8.2689 - val_accuracy: 0.2917\n",
      "Epoch 48/100\n",
      "28/28 [==============================] - 49s 2s/step - loss: 8.3562 - accuracy: 0.2802 - val_loss: 8.2479 - val_accuracy: 0.2878\n",
      "Epoch 49/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 8.3470 - accuracy: 0.2876 - val_loss: 8.2689 - val_accuracy: 0.2839\n",
      "Epoch 50/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 8.2830 - accuracy: 0.2847 - val_loss: 8.2479 - val_accuracy: 0.2852\n",
      "Epoch 51/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 8.3927 - accuracy: 0.2904 - val_loss: 8.2060 - val_accuracy: 0.2904\n",
      "Epoch 52/100\n",
      "28/28 [==============================] - 46s 2s/step - loss: 8.4659 - accuracy: 0.2842 - val_loss: 8.2479 - val_accuracy: 0.2865\n",
      "Epoch 53/100\n",
      "28/28 [==============================] - 46s 2s/step - loss: 8.1916 - accuracy: 0.2836 - val_loss: 8.3109 - val_accuracy: 0.2826\n",
      "Epoch 54/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 8.4750 - accuracy: 0.2853 - val_loss: 8.2269 - val_accuracy: 0.2852\n",
      "Epoch 55/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 8.0819 - accuracy: 0.2842 - val_loss: 8.2899 - val_accuracy: 0.2865\n",
      "Epoch 56/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 8.2190 - accuracy: 0.2785 - val_loss: 8.2269 - val_accuracy: 0.2865\n",
      "Epoch 57/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 8.3562 - accuracy: 0.2898 - val_loss: 8.2269 - val_accuracy: 0.2878\n",
      "Epoch 58/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 8.1642 - accuracy: 0.2847 - val_loss: 8.2689 - val_accuracy: 0.2865\n",
      "Epoch 59/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 8.4933 - accuracy: 0.2791 - val_loss: 8.2479 - val_accuracy: 0.2865\n",
      "Epoch 60/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 8.3287 - accuracy: 0.2813 - val_loss: 8.2060 - val_accuracy: 0.2878\n",
      "Epoch 61/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 7.9905 - accuracy: 0.2898 - val_loss: 8.2060 - val_accuracy: 0.2917\n",
      "Epoch 62/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 8.3105 - accuracy: 0.2796 - val_loss: 8.2269 - val_accuracy: 0.2878\n",
      "Epoch 63/100\n",
      "28/28 [==============================] - 49s 2s/step - loss: 7.8442 - accuracy: 0.2915 - val_loss: 8.2689 - val_accuracy: 0.2852\n",
      "Epoch 64/100\n",
      "28/28 [==============================] - 50s 2s/step - loss: 8.2830 - accuracy: 0.2830 - val_loss: 8.2269 - val_accuracy: 0.2917\n",
      "Epoch 65/100\n",
      "28/28 [==============================] - 50s 2s/step - loss: 8.0728 - accuracy: 0.2847 - val_loss: 8.2479 - val_accuracy: 0.2878\n",
      "Epoch 66/100\n",
      "28/28 [==============================] - 50s 2s/step - loss: 8.1276 - accuracy: 0.2859 - val_loss: 8.2479 - val_accuracy: 0.2878\n",
      "Epoch 67/100\n",
      "28/28 [==============================] - 50s 2s/step - loss: 8.3653 - accuracy: 0.2859 - val_loss: 8.2899 - val_accuracy: 0.2839\n",
      "Epoch 68/100\n",
      "28/28 [==============================] - 49s 2s/step - loss: 8.2373 - accuracy: 0.2870 - val_loss: 8.2269 - val_accuracy: 0.2891\n",
      "Epoch 69/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 8.2648 - accuracy: 0.2808 - val_loss: 8.2689 - val_accuracy: 0.2891\n",
      "Epoch 70/100\n",
      "28/28 [==============================] - 49s 2s/step - loss: 8.0321 - accuracy: 0.2785 - val_loss: 8.2479 - val_accuracy: 0.2865\n",
      "Epoch 72/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 8.1093 - accuracy: 0.2864 - val_loss: 8.2479 - val_accuracy: 0.2878\n",
      "Epoch 73/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 8.1550 - accuracy: 0.2864 - val_loss: 8.2479 - val_accuracy: 0.2852\n",
      "Epoch 74/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 8.1916 - accuracy: 0.2830 - val_loss: 8.2269 - val_accuracy: 0.2852\n",
      "Epoch 75/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 8.1916 - accuracy: 0.2876 - val_loss: 8.2899 - val_accuracy: 0.2826\n",
      "Epoch 76/100\n",
      "28/28 [==============================] - 46s 2s/step - loss: 8.4476 - accuracy: 0.2842 - val_loss: 8.2479 - val_accuracy: 0.2878\n",
      "Epoch 77/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 8.1916 - accuracy: 0.2813 - val_loss: 8.2479 - val_accuracy: 0.2878\n",
      "Epoch 78/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 8.3562 - accuracy: 0.2796 - val_loss: 8.2479 - val_accuracy: 0.2865\n",
      "Epoch 79/100\n",
      "28/28 [==============================] - 50s 2s/step - loss: 8.1642 - accuracy: 0.2842 - val_loss: 8.2269 - val_accuracy: 0.2891\n",
      "Epoch 80/100\n",
      "28/28 [==============================] - 51s 2s/step - loss: 8.1825 - accuracy: 0.2808 - val_loss: 8.2899 - val_accuracy: 0.2839\n",
      "Epoch 81/100\n",
      "28/28 [==============================] - 51s 2s/step - loss: 7.9996 - accuracy: 0.2859 - val_loss: 8.2479 - val_accuracy: 0.2891\n",
      "Epoch 82/100\n",
      "28/28 [==============================] - 50s 2s/step - loss: 8.1459 - accuracy: 0.2791 - val_loss: 8.2269 - val_accuracy: 0.2878\n",
      "Epoch 83/100\n",
      "28/28 [==============================] - 50s 2s/step - loss: 8.2008 - accuracy: 0.2836 - val_loss: 8.2899 - val_accuracy: 0.2865\n",
      "Epoch 84/100\n",
      "28/28 [==============================] - 50s 2s/step - loss: 8.4659 - accuracy: 0.2819 - val_loss: 8.2479 - val_accuracy: 0.2878\n",
      "Epoch 85/100\n",
      "28/28 [==============================] - 49s 2s/step - loss: 8.1368 - accuracy: 0.2808 - val_loss: 8.2899 - val_accuracy: 0.2865\n",
      "Epoch 86/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 8.1550 - accuracy: 0.2876 - val_loss: 8.2899 - val_accuracy: 0.2865\n",
      "Epoch 87/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 8.1916 - accuracy: 0.2825 - val_loss: 8.2689 - val_accuracy: 0.2865\n",
      "Epoch 88/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 8.4019 - accuracy: 0.2915 - val_loss: 8.2060 - val_accuracy: 0.2865\n",
      "Epoch 89/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 7.9905 - accuracy: 0.2887 - val_loss: 8.2269 - val_accuracy: 0.2878\n",
      "Epoch 90/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 8.1733 - accuracy: 0.2859 - val_loss: 8.2269 - val_accuracy: 0.2878\n",
      "Epoch 91/100\n",
      "28/28 [==============================] - 50s 2s/step - loss: 8.2008 - accuracy: 0.2847 - val_loss: 8.2479 - val_accuracy: 0.2904\n",
      "Epoch 92/100\n",
      "28/28 [==============================] - 51s 2s/step - loss: 8.1642 - accuracy: 0.2830 - val_loss: 8.2269 - val_accuracy: 0.2878\n",
      "Epoch 93/100\n",
      "28/28 [==============================] - 49s 2s/step - loss: 8.2373 - accuracy: 0.2842 - val_loss: 8.2269 - val_accuracy: 0.2891\n",
      "Epoch 94/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 8.0545 - accuracy: 0.2825 - val_loss: 8.1850 - val_accuracy: 0.2891\n",
      "Epoch 95/100\n",
      "28/28 [==============================] - 47s 2s/step - loss: 7.9996 - accuracy: 0.2893 - val_loss: 8.2479 - val_accuracy: 0.2839\n",
      "Epoch 96/100\n",
      "28/28 [==============================] - 48s 2s/step - loss: 8.1459 - accuracy: 0.2876 - val_loss: 8.2689 - val_accuracy: 0.2891\n",
      "Epoch 97/100\n",
      "28/28 [==============================] - 49s 2s/step - loss: 8.2190 - accuracy: 0.2887 - val_loss: 8.2269 - val_accuracy: 0.2865\n",
      "Epoch 98/100\n",
      "28/28 [==============================] - 49s 2s/step - loss: 8.2465 - accuracy: 0.2813 - val_loss: 8.2269 - val_accuracy: 0.2878\n",
      "Epoch 99/100\n",
      "28/28 [==============================] - 49s 2s/step - loss: 8.0179 - accuracy: 0.2859 - val_loss: 8.2479 - val_accuracy: 0.2852\n",
      "Epoch 100/100\n",
      "28/28 [==============================] - 49s 2s/step - loss: 8.1459 - accuracy: 0.2796 - val_loss: 8.2689 - val_accuracy: 0.2839\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "history = model.fit(train_gen,\n",
    "                    steps_per_epoch=train_gen.samples // BATCH_SIZE,\n",
    "                    epochs= 100,\n",
    "                    verbose=1,\n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_gen.samples // BATCH_SIZE,\n",
    "                    callbacks=[accuracy_callback])\n",
    "                    #callbacks=[stop_train, accuracy_callback, lr_decay])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/resnet50_v2.h5')\n",
    "model.save_weights('models/resnet50_weights_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plottig history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([1, 0])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6Le05Mb_K9n"
   },
   "source": [
    "## Creating Saliency Maps\n",
    "\n",
    "The problem with creating saliency maps with the custom architecture is that the preloaded model appears as a functional layer. The sublayers in this functional model cannot be called directly. As a result the model gives a \"Graph Disconnected\" error.\n",
    "\n",
    "To workaround this problem a new model is defined without any weights. Weights from pre-trained model are then loaded in this new model and the last conv layer from this model is passed on to the saliency map input and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kiZLVFVl_K9n",
    "outputId": "b791ec99-2467-459a-cd49-5de2ac6764ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficientnetb5 (Functional)  (None, 8, 8, 2048)        28513527  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_7 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 8196      \n",
      "=================================================================\n",
      "Total params: 28,521,723\n",
      "Trainable params: 0\n",
      "Non-trainable params: 28,521,723\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# use this cell if loading a pre-trained model\n",
    "model_pretrained = tf.keras.models.load_model('models/effnet_v1.h5')\n",
    "model_pretrained.trainable = False\n",
    "model_pretrained.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "Rdh2AiK16OO5"
   },
   "outputs": [],
   "source": [
    "# define the classifier layer that will be used to calculate the output\n",
    "classifier_layers = ['global_average_pooling2d_7', 'dense_6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZytdzI2WLrMA",
    "outputId": "01cae257-8312-4414-d716-e33f608d98bd"
   },
   "outputs": [],
   "source": [
    "# define new model\n",
    "base_model = tf.keras.applications.EfficientNetB5(\n",
    "    input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n",
    "    include_top=False,\n",
    "    weights=None\n",
    ")\n",
    "base_model.trainable = False\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SgcEoVNxOAvu",
    "outputId": "8abb4cb6-5375-409d-8503-9d43d30d8799"
   },
   "outputs": [],
   "source": [
    "# load weights in the newly created model\n",
    "base_model.load_weights('models/effnet_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "xKRxjSG2_K9o"
   },
   "outputs": [],
   "source": [
    "# get list of examples to create the heatmaps\n",
    "img_path_list = os.listdir('category _examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "FSvItXQUbPXa"
   },
   "outputs": [],
   "source": [
    "# function to load images to numpy array and normalize\n",
    "def img_to_np(img_path):\n",
    "    # open image using PIL\n",
    "    img_temp = Image.open(img_path)\n",
    "    img_temp = img_temp.resize((IMAGE_SIZE,IMAGE_SIZE), resample=1)\n",
    "\n",
    "    # convert to np array\n",
    "    img_temp = np.array(img_temp).astype(np.float32)\n",
    "    img_temp = img_temp/255.0\n",
    "    \n",
    "    return img_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "7mEzUjup_K9p"
   },
   "outputs": [],
   "source": [
    "# function to cover image with heat map\n",
    "def cover_img(img_path, heatmap):\n",
    "    # Use cv2 to load the original image\n",
    "    img_original = cv2.imread(img_path)\n",
    "\n",
    "    # set the intensity map at 0.5\n",
    "    INTENSITY = 0.5\n",
    "\n",
    "    # Resize the heatmap to have the same size as the original image\n",
    "    heatmap_resized = cv2.resize(heatmap, (img_original.shape[1], img_original.shape[0]))\n",
    "\n",
    "    # Convert the heatmap to RGB\n",
    "    heatmap_resized = np.uint8(255 * heatmap_resized)\n",
    "\n",
    "    # We apply the heatmap to the original image\n",
    "    heatmap_resized = cv2.applyColorMap(heatmap_resized, cv2.COLORMAP_JET)\n",
    "\n",
    "    # processed image\n",
    "    img_new = heatmap_resized * INTENSITY + img_original\n",
    "\n",
    "    return img_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLwHuKNY_K9p"
   },
   "source": [
    "### Get Gradient on Images\n",
    "Finally test the functions on all images. We will also save all the bounding boxes with the file names in a dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "UZS3kYnx_K9q"
   },
   "outputs": [],
   "source": [
    "# import needed modules\n",
    "from tensorflow import GradientTape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "w7IiI8v57EHP"
   },
   "outputs": [],
   "source": [
    "# last conv layer of the base model\n",
    "last_conv_layer_1 = 'top_conv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "pGeXnMpH3BiM"
   },
   "outputs": [],
   "source": [
    "def make_gradcam_heatmap(\n",
    "    img_array, model, pre_trained_model, last_conv_layer_name, classifier_layer_names\n",
    "):\n",
    "    # First, we create a model that maps the input image to the activations\n",
    "    # of the last conv layer\n",
    "    last_conv_layer = model.get_layer(last_conv_layer_name)\n",
    "    last_conv_layer_model = tf.keras.Model(model.inputs, last_conv_layer.output)\n",
    "\n",
    "    # Second, we create a model that maps the activations of the last conv\n",
    "    # layer to the final class predictions\n",
    "    classifier_input = tf.keras.Input(shape=last_conv_layer.output.shape[1:])\n",
    "    x = classifier_input\n",
    "    for layer_name in classifier_layer_names:\n",
    "        x = pre_trained_model.get_layer(layer_name)(x)\n",
    "    classifier_model = tf.keras.Model(classifier_input, x)\n",
    "\n",
    "    # Then, we compute the gradient of the top predicted class for our input image\n",
    "    # with respect to the activations of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Compute activations of the last conv layer and make the tape watch it\n",
    "        last_conv_layer_output = last_conv_layer_model(img_array)\n",
    "        tape.watch(last_conv_layer_output)\n",
    "        # Compute class predictions\n",
    "        preds = classifier_model(last_conv_layer_output)\n",
    "        top_pred_index = tf.argmax(preds[0])\n",
    "        top_class_channel = preds[:, top_pred_index]\n",
    "\n",
    "    # This is the gradient of the top predicted class with regard to\n",
    "    # the output feature map of the last conv layer\n",
    "    grads = tape.gradient(top_class_channel, last_conv_layer_output)\n",
    "\n",
    "    # This is a vector where each entry is the mean intensity of the gradient\n",
    "    # over a specific feature map channel\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # We multiply each channel in the feature map array\n",
    "    # by \"how important this channel is\" with regard to the top predicted class\n",
    "    last_conv_layer_output = last_conv_layer_output.numpy()[0]\n",
    "    pooled_grads = pooled_grads.numpy()\n",
    "    for i in range(pooled_grads.shape[-1]):\n",
    "        last_conv_layer_output[:, :, i] *= pooled_grads[i]\n",
    "\n",
    "    # The channel-wise mean of the resulting feature map\n",
    "    # is our heatmap of class activation\n",
    "    heatmap = np.mean(last_conv_layer_output, axis=-1)\n",
    "\n",
    "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
    "    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "fjLH0kf18GmR",
    "outputId": "7cda8e5a-0e9b-4d44-fd3e-72f6cd3a8109"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD3CAYAAADbsCLdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAU7klEQVR4nO3df4xdZZ3H8fen01+0tFS2CNhWQe2yUVYBseiyywr4oyAL2cRo8VdgNZVEDCQmirvJkt3NbrIxMWJEmgYqEhFWC+x23UJlI8gagW0LCJRStxS1Y4FaUVp+DjPz3T/uGXI7zNz7DH1O7zn3fF7JSefOPf3cp5N+5znnOc95jiICM+t/03rdADM7OFzsZg3hYjdrCBe7WUO42M0awsVu1hAudrMKkrRG0m5JD0/yviR9Q9J2SQ9KOqlbpovdrJquBZZ3eP8sYGmxrQSu6hboYjeroIi4C3i6wy7nAddFyz3AAklHd8p0sZvV0yJgZ9vrweJ7k5peanPMGuJDp8+N3z09krTv5gdf2gK82Pat1RGxeoofqQm+13Huu4vdLIM9T49w74bFSfvOOPqxFyPi5AP8yEFgSdvrxcCuTn/Bh/FmWQQjMZq0ZbIO+HQxKv8e4JmIeKLTX3DPbpZBAKOdj6KnRNINwPuAhZIGgcuBGQARsQpYD5wNbAeeBy7sluliN8sgCF6OtHP2pLyI87u8H8Dnp5LpYjfLJGfPXoaenbNLWi5pWzED6LKMuR1nHh1A7hJJd0jaKmmLpEsy5c6W9L+Sfl7k/kOO3Lb8AUn3S/ph5txfSnpI0gOSNmXMXSBpraRHi5/1ezPlHle0dWzbK+nSHNnQOowfIZK2XulJzy5pALgS+ACtUcWNktZFxCMZ4q8FvglclyGr3TDwxYi4T9I8YLOk2zO0+SXgjIh4VtIM4KeSbi0mSuRwCbAVmJ8pr93pEbEnc+YVwG0R8RFJM4E5OUIjYhtwArzy/+83wC05sse4Z5/YMmB7ROyIiCHgRlozgg5Ywsyj15r7RETcV3y9j1YBdZzEkJgbEfFs8XJGsWX5XyNpMfBh4OoceWWTNB84DbgGICKGIuIPJXzUmcBjEfGrXIEBjEQkbb3Sq2Kf8uyfKpF0DHAicG+mvAFJDwC7gdsjIksu8HXgS0C26z1tAviRpM2SVmbKfDPwW+DbxanH1ZLmZsputwK4IXfoaOLWK70q9inP/qkKSYcCNwGXRsTeHJkRMRIRJ9CaGLFM0vEHminpHGB3RGw+0KxJnBoRJ9G6IePzkk7LkDkdOAm4KiJOBJ4Dso3nABSnBucCP8iZG4nn6708Z+9VsU959k8VFOfUNwHXR8TNufOLQ9Y76Xy3U6pTgXMl/ZLWadIZkr6bIReAiNhV/Lmb1rnvsgyxg8Bg25HNWlrFn9NZwH0R8VTO0Ah4OXHrlV4V+0ZgqaRji9+0K2jNCKosSaJ1Lrk1Ir6WMfcISQuKrw8B3g88eqC5EfGViFgcEcfQ+vn+OCI+eaC5AJLmFoOUFIfZHwQO+OpHRDwJ7JR0XPGtM4Ecg7btzqeEQ3gQI4lbr/RkND4ihiVdDGwABoA1EbElR/ZEM48i4poM0acCnwIeKs6vAf42ItYfYO7RwHeKEeJpwPcjIutlshIcCdzS+v3HdOB7EXFbpuwvANcXncAOEmaGpZI0h9YVoM/lyhwTwGjFT0Tlh0SYHbjj3zEzvv9fRyTt+/Y37tqc4UaYKfMMOrMMWpNqeneInsLFbpbJaLjYzfqee3azhgjEyzHQ62Z01PPFKzLOvqp1bpnZdcstM7us3LGevcqX3npe7LSWwXVuudl1yy0zu6RcMRLTkrZe8WG8WQatlWqq0HdOrpRin6lZMZu0+xdmM4f5Ojz7xf665ZaZPeXcObPTcmcexvy5b0jOjYH0Ypg1ewHz5i9Ozp72ctoqMbNnzOewQ45Oyn3h5WcYGn4++bi7kQN0s5nLKTqzjGg7CPT2A74PZ0IvHzarlFyAWU/sy55592NrkveNUE8P0VP4MN4sk9Em9uxmTROIoah2OVW7dWY10dgBOrMmGvF0WbP+F4gR9+xmzTBa8dH4pNaVtca7Wb9oTZedlrT1SteeveQ13s36Qh1uhEk5jH9ljXcASWNrvLvYzQoR9MWkmonWeD9l/E7F3UQroTU906xZ1BeTapLWeI+I1cBqoLS542ZV1XoiTP179lqu8W52sPXDpbdX1nin9TC8FcDHS22VWc0Eqv8adGWu8W7WT/qhZ6d4EMKBPgzBrG/1y6U3M+ui9USYPujZzay7qq9UU+1fRWY1ESFGY1rSlqLbFHVJh0n6T0k/l7RFUtdn4rlnN8sk13X2xCnqnwceiYi/knQEsE3S9RExNFluKcWuQ2Yz7a1/kj33hSXzsmeOmXXrxtKyyzDt+Pw/3zGDp88vJXfB9rRFIV+L2Y9P+n/8tZvCQ09bi1dkO4xPmaIewLziUeKHAk8Dw51C3bObZTGlBScXStrU9np1MQN1TMoU9W8C62hNcJsHfCwiRjt9qIvdLIOAqVx629Plkc0pU9Q/BDwAnAG8Bbhd0v9ExN7JQj1AZ5bB2Ay6lC1ByhT1C4Gbo2U78DjQ8dzOxW6WySjTkrYEr0xRlzST1hT1deP2+TVwJoCkI4HjgB2dQn0Yb5ZB6372PAN0k01Rl3RR8f4q4J+AayU9ROuw/8sRsadTrovdLJOcN8JMNEW9KPKxr3cBH5xKpovdLIPWOXu1z4pd7GaZVH26bMqCk2uAc4DdEVHOE//Mai4Qw6PVvust5bjjWmB5ye0wq73RYh26bluvpCxecZekYw5CW8xqK+dofFmynbPvt7rsjHLmVptVWWMG6NpXlz1szhu8uqw1Sl+sQWdmafph3Xgz66K1LFW1i73rSYakG4C7geMkDUr6TPnNMquZaF16S9l6JWU0/vyD0RCzOsu8eEUpfBhvlknVD+Nd7GYZ1OGc3cVulomL3awBfJ3drCkChpsyg24/w8NM+90fssc+c9rh2TPH7P3ae0rJXfq9Z0vJ3fvmQ0vJBZj+Qjm5z17wTDnBwN4735A9c+j6Gcn7+pzdrEFc7GYN4HN2swYJF7tZM3gGnVkDRPic3awhxMhoEy+9mTVQ1c/ZU25xXSLpDklbi4e+X3IwGmZWJ2PX2TM9660UKT37MPDFiLhP0jxgs6Tbxz0Y3qzZYkqPc++JlPvZnwCeKL7eJ2krredHu9jN2vTVaHyxpPSJwL2ltMaspoLqn7MnF7ukQ4GbgEsneuD7fktJD5Q3b9usmvpkBp2kGbQK/fqIuHmiffZbSnrm6yt+9mKW3+hozYtdkoBrgK0R8bXym2RWPxHVP4xPmQVwKvAp4AxJDxTb2SW3y6x2an/pLSJ+ChUfZjSrgNpfejOzNFU/jHexm2UQqPLFXu2Z+2Y1EolbCknLJW2TtF3SZZPs875iDG2LpJ90y3TPbpZDQGS69CZpALgS+AAwCGyUtK59irqkBcC3gOUR8WtJr++W657dLJMIJW0JlgHbI2JHRAwBNwLnjdvn48DNEfHr1mfH7m6h5fTsATE6mj322TeWN9x5xqkPlZJ79+53lpJb5qrFQwvK+Tn/3XE/LiUX4B+fOSd75ugtU/s5ZByNXwTsbHs9CJwybp8/BmZIuhOYB1wREdd1CvVhvFkGU5wbv1DSprbXq4sZqGMmChr/q2Q68C7gTOAQ4G5J90TELyb7UBe7WQ4BpBf7nog4ucP7g8CStteLgV0T7LMnIp4DnpN0F/BOYNJi9zm7WSYRaVuCjcBSScdKmgmsANaN2+c/gL+QNF3SHFqH+Vs7hbpnN8sl0zl7RAxLuhjYAAwAayJii6SLivdXRcRWSbcBDwKjwNUR8XCnXBe7WRbKdukNICLWA+vHfW/VuNdfBb6amuliN8uhBne9udjNcvGNMGZNUfOeXdJs4C5gVrH/2oi4vOyGmdVOH/TsLwFnRMSzxfJUP5V0a0TcU3LbzOql7sUeEQE8W7ycUWwV/2eZHWQZb4QpS9KkGkkDkh4AdgO3R8SrlpKWtFLSJkmbhkZfyNxMsxrIeY9rCZKKPSJGIuIEWtP2lkk6foJ9VkfEyRFx8sxph2RuplkNhNK2HpnSdNmI+ANwJ7C8jMaY1ZkibeuVlAc7HlHcKI+kQ4D3A4+W3C6zekk9hO9hsaeMxh8NfKdYPWMa8P2I+GG5zTKrm94eoqdIGY1/kNbz3cysk4pfo/IMOrNc8i/OlJWL3SyHqS1e0RMudrNMejnSnsLFbpZLI4t9YBrMm5s99u3v3ZE9c8wVi+4oJffyjz5fSu7a+99VSi7AjLlDpeReML/rasev2Y/e8nj+zFkvZc/sJffsZpn4MN6sKTxAZ9YAgS+9mTWFD+PNmsLFbtYQLnaz/tfr21dTuNjNcqn4aHzy4hXF0lT3S/LtrWYT6YP72cdcQuvBcfNLaotZranil95SF5xcDHwYuLrc5pjVVOKSVJVelqrwdeBLdJg2sN/qsiNeXdYaqOKH8Slr0J0D7I6IzZ3222912QGvLmsNVPFiTzlnPxU4V9LZwGxgvqTvRsQny22aWb1U/dJb1549Ir4SEYsj4hhgBfBjF7pZ/fg6u1kuFe/Zp1TsEXEnrYdEmFm7qP6lN/fsZrn0U89uZhMTfTBAZ2aJMl56k7Rc0jZJ2yVd1mG/d0sakfSRbpkudrMcMs6gKx61diVwFvA24HxJb5tkv38FNqQ00cVulku+nn0ZsD0idkTEEHAjcN4E+30BuAlIWra3lHP2lxbOYPvfHJU99xMLfpI9c8ymoZml5H71qPtLyV07cnIpuQADWw8tJfddcz9aSi7AvudmZ898for/JzKOxi8Cdra9HgRO2e+zpEXAXwNnAO9OCfUAnVku6QN0CyVtanu9OiJWt72e6Mb48elfB74cESNS2n30LnazHKY2731PRHQ6NBsElrS9XgzsGrfPycCNRaEvBM6WNBwR/z5ZqIvdLJOMl942AkslHQv8htY09Y+37xARx77yudK1wA87FTq42M3yyVTsETEs6WJao+wDwJqI2CLpouL9Va8l18VulknOSTURsR5YP+57ExZ5RFyQkuliN8ul4jPoXOxmGfR6yakUScUu6ZfAPmAEGO4ykmjWTP1Q7IXTI2JPaS0xq7m+6NnNLEHFiz11bnwAP5K0WdLKMhtkVlt9sOAkwKkRsUvS64HbJT0aEXe171D8ElgJMH3B6zI306ziajBAl9SzR8Su4s/dwC207soZv88rS0kPzJ2bt5VmdVDxnj1l3fi5kuaNfQ18EHi47IaZ1Y1G07ZeSTmMPxK4pZhwPx34XkTcVmqrzGqo6ofxXYs9InYA7zwIbTGrrx4foqfwpTezXFzsZv2vDqvLutjNcnGxmzWDotrV7mI3y6Gpj3+a9jLM3ZW2CN5U/Nv/nZQ9c8xb//SpUnIv35d/lV0AzR4pJRfgxTeV00MN75tTSi7AUT+YlT3zyaenuNJ6tTt29+xmuXiAzqwpXOxmDVCDG2Fc7Ga5uNjN+p8n1Zg1iEarXe0udrMcfCOMWXNUfVJN0qwBSQskrZX0qKStkt5bdsPMaqfiK9Wk9uxXALdFxEckzQTKmwplVlO1H6CTNB84DbgAICKGgKFym2VWMwFU/EaYlMP4NwO/Bb4t6X5JVxdr0e1H0kpJmyRtGn7huewNNau6qq9Bl1Ls04GTgKsi4kTgOeCy8Tu1ry47/RCvLmvNMnadPWXrlZRiHwQGI+Le4vVaWsVvZmMi0rce6VrsEfEksFPSccW3zgQeKbVVZjVU9Z49dTT+C8D1xUj8DuDC8ppkVlPVHp9LK/aIeADwY5rNOqj9pTczSxCA58abNUNfTJc1swQZR+MlLZe0TdJ2Sa+61C3pE5IeLLafSer61Cb37GaZ5DpnlzQAXAl8gNal742S1kVE+1Wwx4G/jIjfSzoLWA2c0inXPbtZDqk3waT9QlgGbI+IHcX09BuB8/b7uIifRcTvi5f3AIu7hZbSs8/YO8zRd/wue+7uFw/PnjnmXzZ/rJTcOU+VM2hz5AvlDQa9+Lpy+oDDHy2vzTP3PJM9c+Cl9OW6WzPokv99CyVtanu9OiJWt71eBOxsez1I5177M8Ct3T7Uh/FmuaQP0O2JiE6Xsid66MKEv0kknU6r2P+824e62M0yyfj4p0FgSdvrxcCuV32e9A7gauCsiOh6KO1zdrMcIlrX2VO27jYCSyUdW8xaXQGsa99B0huBm4FPRcQvUkLds5tlkms0PiKGJV0MbAAGgDURsUXSRcX7q4C/B/4I+JYkgOEupwYudrNsMt7RFhHrgfXjvreq7evPAp+dSqaL3SyHpj7F1ayR6r4slaTjJD3Qtu2VdOlBaJtZvdR9ddmI2AacAK9M4/sNcEu5zTKrn4yX3kox1cP4M4HHIuJXZTTGrLYCGOmvYl8B3DDRG5JWAisBZs+Yf4DNMqsXEZXv2ZMn1RQX988FfjDR++2ry84c8DMkrIEqvuDkVHr2s4D7IuKpshpjVmsV79mnUuznM8khvFnjBVO5EaYnkopd0hxaN9J/rtzmmNVX1c/ZU1eXfZ7WPFwzm0w/FLuZdREBo9U+jnexm+VS7Vp3sZvl0hfn7GaWwMVu1gBNfSLM3hef3LPh4X9OnT+/ENiTtOfDU2pGeu7UlJVbZnbdcsvMnkrum9Jjezs7LkUpxR4RR6TuK2lTt+V0Xou65ZaZXbfcMrPLbHMji92scQIYqfZwvIvdLIuAcLF3s7r7Lo3ILTO7brllZpfX5oofxisq3kCzOjhs5pHxZ0edn7TvbTuv2FzauEEHVejZzfpDxTtOF7tZLi52swaIgJH0p772govdLBf37GYN4WI3a4LkJ7T2jIvdLIeA8KQas4Zwz27WED5nN2sAX3oza47wgpNmTdDQxSvMGqcGy1IlP9jRzLqI0bQtgaTlkrZJ2i7psgnel6RvFO8/KOmkbpnu2c0yCCAy9eySBoAraT1ybRDYKGldRDzStttZwNJiOwW4qvhzUu7ZzXKIyNmzLwO2R8SOiBgCbgTOG7fPecB10XIPsEDS0Z1C3bObZRL5Lr0tAna2vR7k1b32RPssAp6YLNTFbpbBPn6/4b9j7cLE3WdL2tT2enVEtC+XpQn+zvhzhJR99uNiN8sgIpZnjBsElrS9Xgzseg377Mfn7GbVsxFYKulYSTOBFcC6cfusAz5djMq/B3gmIiY9hAf37GaVExHDki4GNgADwJqI2CLpouL9VcB64GxgO/A8cGG3XK8ua9YQPow3awgXu1lDuNjNGsLFbtYQLnazhnCxmzWEi92sIVzsZg3x/5+6mk8cKyBNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for img in img_path_list:\n",
    "    img_path = os.path.join('category _examples',img)\n",
    "    img_temp = img_to_np(img_path)\n",
    "    img_temp = np.expand_dims(img_temp, axis=0)\n",
    "    \n",
    "    htmp = make_gradcam_heatmap(img_temp, base_model, model_pretrained, last_conv_layer_1, classifier_layers)\n",
    "    # the reshape value will very if a different model is used since the shape \n",
    "    # depends on the output of the final conv layer, so make changes accordingly\n",
    "    heatmap = htmp.reshape((8, 8))\n",
    "    plt.matshow(heatmap)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "    # save image with heatmap cover\n",
    "    #plt.imshow(cover_img(img_path, htmp), cmap='gray', interpolation='bicubic')\n",
    "    cv2.imwrite(os.path.join('img_heatmap', img), cover_img(img_path, htmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlain images\n",
    "<img src=\"img_heatmap/dbr1.jpg\" alt=\"diabetic retinopathy\" width=\"200\"/>\n",
    "<img src=\"img_heatmap/dbr2.jpg\" alt=\"diabetic retinopathy\" width=\"200\"/>\n",
    "<img src=\"img_heatmap/glc1.jpg\" alt=\"diabetic retinopathy\" width=\"200\"/>\n",
    "<img src=\"img_heatmap/glc2.jpg\" alt=\"diabetic retinopathy\" width=\"200\"/>\n",
    "<img src=\"img_heatmap/normal1.jpg\" alt=\"diabetic retinopathy\" width=\"200\"/>\n",
    "<img src=\"img_heatmap/normal2.jpg\" alt=\"diabetic retinopathy\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4UyLYQX9JpD"
   },
   "source": [
    "# Label Propogation\n",
    "\n",
    "### Setting up Label Propogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhswtM6oAo--"
   },
   "source": [
    "## Task 3: Using the unlabelled data set in the 'test' folder augment the training data (semi-supervised learning) and report the variation in classification performance on test data set.(15 points)\n",
    "[You may use any method of your choice, one possible way is mentioned below.] \n",
    "\n",
    "```\n",
    "Hint: \n",
    "a. Train a model using the 'train' split.\n",
    "b. Pass the unlabelled images through the trained model and retrieve the dense layer feature prior to classification layer. Using this dense layer as representative of the image, apply label propagation to retrieve labels correspndng to the unbalelled data.\n",
    "c. Next, concatenate the train data with the unlabelled data (that has now been self labelled) and retrain the network.\n",
    "d. Report classification performance on test data\n",
    "Use the unlabelled test data  to improve classification performance by using a semi-supervised label-propagation/self-labelling approach. (20 points)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qNJoybPw4zML",
    "outputId": "1dd12c57-fa14-467e-d1fd-014c6363c8ff"
   },
   "outputs": [],
   "source": [
    "# load trained model\n",
    "model_pretrained = tf.keras.models.load_model('models/resnet50_v1.h5')\n",
    "model_pretrained.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Functional)        (None, 7, 7, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_11  (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 24,638,852\n",
      "Trainable params: 0\n",
      "Non-trainable params: 24,638,852\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_pretrained.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "id": "gE85ex-AE-VA"
   },
   "outputs": [],
   "source": [
    "last_dense_layer = 'dense_38'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "id": "6doiG7zDA5q5"
   },
   "outputs": [],
   "source": [
    "# setup to get the output from the last dense layer\n",
    "last_layer = model_pretrained.get_layer(last_dense_layer)\n",
    "last_conv_layer_model = tf.keras.Model(model_pretrained.inputs, last_layer.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sDqVpkuHn1c6",
    "outputId": "4d703dfc-31b0-403f-b19a-df106530ba3e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 7)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a train array for passing to knn\n",
    "pred_train = np.array([ 'name', 'p1', 'p2', 'p3', 'p4', 'labels', 'label_num'])\n",
    "pred_train = np.expand_dims(pred_train, axis=0)\n",
    "pred_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_original_dir = 'Data/train/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "id": "bcE-pGiwCyWq"
   },
   "outputs": [],
   "source": [
    "# iterating over the image folder to collect predictions, names and labels\n",
    "for pic in os.listdir(train_original_dir):\n",
    "    # get image array\n",
    "    im_temp = img_to_np(os.path.join(train_original_dir, pic))\n",
    "    im_temp = np.expand_dims(im_temp, axis=0)\n",
    "\n",
    "    pred_temp = []\n",
    "\n",
    "    # get image label\n",
    "    max_val = 1\n",
    "    labels = [i for i in train_labels.loc[pic].values]\n",
    "\n",
    "    if labels[1] == 1:\n",
    "        pred_temp.append(pic)\n",
    "        for j in last_conv_layer_model.predict(im_temp)[0]:\n",
    "            pred_temp.append(j)\n",
    "        pred_temp.append('diabetic retinopathy')\n",
    "        pred_temp.append(1)\n",
    "    elif labels[2] == 1:\n",
    "        pred_temp.append(pic)\n",
    "        for j in last_conv_layer_model.predict(im_temp)[0]:\n",
    "            pred_temp.append(j)\n",
    "        pred_temp.append('glaucoma')\n",
    "        pred_temp.append(2)\n",
    "    elif labels[6] == 1:\n",
    "        pred_temp.append(pic)\n",
    "        for j in last_conv_layer_model.predict(im_temp)[0]:\n",
    "            pred_temp.append(j)\n",
    "        pred_temp.append('normal')\n",
    "        pred_temp.append(3)\n",
    "    else:\n",
    "        pred_temp.append(pic)\n",
    "        for j in last_conv_layer_model.predict(im_temp)[0]:\n",
    "            pred_temp.append(j)\n",
    "        pred_temp.append('other')\n",
    "        pred_temp.append(4)\n",
    "\n",
    "    pred_train = np.append(pred_train, np.expand_dims(np.array(pred_temp), axis=0), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "id": "Wt8uMZ7fxB7b"
   },
   "outputs": [],
   "source": [
    "# extract specific columns from np array\n",
    "# use X = data[:, [1, 9]]\n",
    "X_train = pred_train[1:,[1, 2, 3, 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "id": "XxKhqb6pxtbb"
   },
   "outputs": [],
   "source": [
    "# extract targets\n",
    "y_train = pred_train[1:, [6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "id": "IzwTWqqLW1VZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "id": "q43H5DMumQJl"
   },
   "outputs": [],
   "source": [
    "# create a label propagation model\n",
    "prop = KNN(n_neighbors=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5z0xg-rZmxMX",
    "outputId": "27538494-26a1-4e49-8ca3-2a7e90a75709"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=4)"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "prop.fit(X_train.astype(np.float32), y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rs8IK0rFrAYQ",
    "outputId": "18555cba-5d95-4153-88ec-c781d6422bee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict labels for the test folder\n",
    "# creating a test array for passing to knn model\n",
    "pred_test = np.array(['name', 'labels', 'label_num'])\n",
    "pred_test = np.expand_dims(pred_test, axis=0)\n",
    "pred_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "id": "l1Zjb8B3rfNY"
   },
   "outputs": [],
   "source": [
    "# create labels for the test images\n",
    "# iterating over the image folder to collect predictions, names and labels\n",
    "for pic in os.listdir(test_img_dir):\n",
    "    # get image array\n",
    "    im_temp = img_to_np(os.path.join(test_img_dir, pic))\n",
    "    im_temp = np.expand_dims(im_temp, axis=0)\n",
    "\n",
    "    pred_temp = []\n",
    "\n",
    "    # get image predited labels\n",
    "    label_predicted = prop.predict(np.expand_dims(\n",
    "        last_conv_layer_model.predict(im_temp)[0], axis=0))\n",
    "  \n",
    "    # append labels to the new array\n",
    "    if int(label_predicted[0]) == 1:\n",
    "        pred_temp.append(pic)\n",
    "        pred_temp.append('diabetic retinopathy')\n",
    "        pred_temp.append(1)\n",
    "\n",
    "    elif int(label_predicted[0]) == 2:\n",
    "        pred_temp.append(pic)\n",
    "        pred_temp.append('galucoma')\n",
    "        pred_temp.append(2)\n",
    "\n",
    "    elif int(label_predicted[0]) == 3:\n",
    "        pred_temp.append(pic)\n",
    "        pred_temp.append('normal')\n",
    "        pred_temp.append(3)\n",
    "\n",
    "    else:\n",
    "        pred_temp.append(pic)\n",
    "        pred_temp.append('other')\n",
    "        pred_temp.append(4)\n",
    "\n",
    "    pred_test = np.append(pred_test, np.expand_dims(np.array(pred_temp), axis=0), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_LDHBe_va0c"
   },
   "source": [
    "### Append the test data to train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350, 3)"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test_ = pred_test[1:]\n",
    "pred_test_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "id": "27J4yQuxvd18"
   },
   "outputs": [],
   "source": [
    "# create new folders with images\n",
    "new_image_dir = 'Data/separated_pics_aug'\n",
    "\n",
    "for row in pred_test_:\n",
    "    file_name = row[0]\n",
    "    label_num = row[2]\n",
    "\n",
    "    if label_num == 1:\n",
    "        shutil.copy(os.path.join(test_img_dir, file_name), \n",
    "                    os.path.join(new_image_dir, 'diabetic retinopathy/'+file_name))\n",
    "  \n",
    "    elif label_num == 2:\n",
    "        shutil.copy(os.path.join(test_img_dir, file_name), \n",
    "                    os.path.join(new_image_dir, 'glaucoma/'+file_name))\n",
    "    \n",
    "    elif label_num == 3:\n",
    "        shutil.copy(os.path.join(test_img_dir, file_name), \n",
    "                    os.path.join(new_image_dir, 'normal/'+file_name))\n",
    "  \n",
    "    else:\n",
    "        shutil.copy(os.path.join(test_img_dir, file_name), \n",
    "                    os.path.join(new_image_dir, 'other/'+file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDdUACUU_aOJ"
   },
   "source": [
    "## Train on Concatenated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "755\n",
      "755\n",
      "578\n",
      "578\n",
      "525\n",
      "525\n",
      "749\n",
      "1099\n"
     ]
    }
   ],
   "source": [
    "# check if the data has been added\n",
    "print(len(os.listdir('Data/separated_pics_train/diabetic retinopathy')))\n",
    "print(len(os.listdir('Data/separated_pics_aug/diabetic retinopathy')))\n",
    "print(len(os.listdir('Data/separated_pics_train/glaucoma')))\n",
    "print(len(os.listdir('Data/separated_pics_aug/glaucoma')))\n",
    "print(len(os.listdir('Data/separated_pics_train/normal')))\n",
    "print(len(os.listdir('Data/separated_pics_aug/normal')))\n",
    "print(len(os.listdir('Data/separated_pics_train/other')))\n",
    "print(len(os.listdir('Data/separated_pics_aug/other')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "id": "c2FOS_XA3Kgb"
   },
   "outputs": [],
   "source": [
    "# define image size and channels\n",
    "IMAGE_SIZE = 256\n",
    "CHANNELS = 3\n",
    "\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "id": "3WoCkfqj6pNU"
   },
   "outputs": [],
   "source": [
    "# training image dir\n",
    "train_img_dir = os.path.join(data_path, 'separated_pics_aug')\n",
    "\n",
    "# testing image dir\n",
    "test_img_dir = 'Data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "id": "ivs-oNfSwdnW"
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=10.,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    channel_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    brightness_range=(-0.5,0.5),\n",
    "    fill_mode='constant',\n",
    "    data_format=\"channels_last\",\n",
    "    #preprocessing_function=tf.keras.applications.vgg19.preprocess_input,\n",
    "    preprocessing_function=contrast,\n",
    "    validation_split = 0.3,\n",
    "    rescale=1.0/255.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "id": "nsUJYt0mytWI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2072 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# create training data\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    directory=train_img_dir,\n",
    "    shuffle=True, \n",
    "    target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    subset='training'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 885 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# create training data\n",
    "val_gen = train_datagen.flow_from_directory(\n",
    "    directory=train_img_dir,\n",
    "    shuffle=True, \n",
    "    target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oR8iRNCIzpvy"
   },
   "source": [
    "### Creating callback for saving models and stopping training if certain accuracy is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ev1qOfkS0HYb"
   },
   "source": [
    "Download application from the tf.keras.application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "id": "3y2sgtyW0DCn"
   },
   "outputs": [],
   "source": [
    "# importing resnet50 from tensorflow applications\n",
    "base_model = tf.keras.applications.ResNet50(\n",
    "    input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "id": "_gcyfhHO01y0"
   },
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "#base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "id": "utefTjwS1dQf"
   },
   "outputs": [],
   "source": [
    "# select a layer to start optimizing from, \n",
    "# here we will train only the later half of the model\n",
    "train_layer = base_model.get_layer('conv4_block4_1_conv')\n",
    "\n",
    "# get train_layer index\n",
    "layer_index = base_model.layers.index(train_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "id": "kI5lDdNv2-nu"
   },
   "outputs": [],
   "source": [
    "# make layers before the training layer non trainable\n",
    "for layer in base_model.layers[:layer_index]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skipping the final activation layer and just getting the logits as these will be used for the final part of the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "id": "EnLvN2os25WE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Functional)        (None, 7, 7, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_16  (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 4)                 8196      \n",
      "=================================================================\n",
      "Total params: 23,595,908\n",
      "Trainable params: 18,340,356\n",
      "Non-trainable params: 5,255,552\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 1.3753 - accuracy: 0.3327INFO:tensorflow:Assets written to: models/acc_ckpts/assets\n",
      "32/32 [==============================] - 107s 3s/step - loss: 1.3753 - accuracy: 0.3327 - val_loss: 435.1318 - val_accuracy: 0.3678\n",
      "Epoch 2/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 1.3614 - accuracy: 0.3675INFO:tensorflow:Assets written to: models/acc_ckpts/assets\n",
      "32/32 [==============================] - 90s 3s/step - loss: 1.3614 - accuracy: 0.3675 - val_loss: 29.0607 - val_accuracy: 0.3798\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - 68s 2s/step - loss: 1.3663 - accuracy: 0.3635 - val_loss: 13.4626 - val_accuracy: 0.3738\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 1.3590 - accuracy: 0.3635INFO:tensorflow:Assets written to: models/acc_ckpts/assets\n",
      "32/32 [==============================] - 90s 3s/step - loss: 1.3590 - accuracy: 0.3635 - val_loss: 2.0717 - val_accuracy: 0.2464\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 1.4009 - accuracy: 0.3277"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "history = model.fit(train_gen,\n",
    "                    steps_per_epoch=train_gen.samples // BATCH_SIZE,\n",
    "                    epochs= 100,\n",
    "                    verbose=1,\n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_gen.samples // BATCH_SIZE,\n",
    "                    callbacks=[accuracy_callback])\n",
    "                    #callbacks=[stop_train, accuracy_callback, lr_decay])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plottig history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([1, 0])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/resnet50_augumented.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Rana ML_for_Computer_Vision.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
