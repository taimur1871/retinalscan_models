{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQ8_RRcaZNXV"
   },
   "source": [
    "## This assignment is designed for automated pathology detection for Medical Images in a relalistic setup, i.e. each image may have multiple pathologies/disorders. \n",
    "### The goal, for you as an MLE, is to design models and methods to predictively detect pathological images and explain the pathology sites in the image data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eK8M8sjWZVg9"
   },
   "source": [
    "## Data for this assignment is taken from a Kaggle contest: https://www.kaggle.com/c/vietai-advance-course-retinal-disease-detection/overview\n",
    "Explanation of the data set:\n",
    "The training data set contains 3435 retinal images that represent multiple pathological disorders. The patholgy classes and corresponding labels are: included in 'train.csv' file and each image can have more than one class category (multiple pathologies).\n",
    "The labels for each image are\n",
    "\n",
    "```\n",
    "-opacity (0), \n",
    "-diabetic retinopathy (1), \n",
    "-glaucoma (2),\n",
    "-macular edema (3),\n",
    "-macular degeneration (4),\n",
    "-retinal vascular occlusion (5)\n",
    "-normal (6)\n",
    "```\n",
    "The test data set contains 350 unlabelled images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhDeT7B2Zk2S"
   },
   "source": [
    "# For this assignment, you are working with specialists for Diabetic Retinopathy and Glaucoma only, and your client is interested in a predictive learning model along with feature explanability and self-learning for Diabetic Retinopathy and Glaucoma vs. Normal images.\n",
    "# Design models and methods for the following tasks. Each task should be accompanied by code, plots/images (if applicable), tables (if applicable) and text:\n",
    "## Task 1: Build a classification model for Diabetic Retinopathy and Glaucoma vs normal images. You may consider multi-class classification vs. all-vs-one classification. Clearly state your choice and share details of your model, paremeters and hyper-paramaterization pprocess. (60 points)\n",
    "```\n",
    "a. Perform 70/30 data split and report performance scores on the test data set.\n",
    "b. You can choose to apply any data augmentation strategy. \n",
    "Explain your methods and rationale behind parameter selection.\n",
    "c. Show Training-validation curves to ensure overfitting and underfitting is avoided.\n",
    "```\n",
    "## Task 2: Visualize the heatmap/saliency/features using any method of your choice to demonstrate what regions of interest contribute to Diabetic Retinopathy and Glaucoma, respectively. (25 points)\n",
    "```\n",
    "Submit images/folder of images with heatmaps/features aligned on top of the images, or corresponding bounding boxes, and report what regions of interest in your opinion represent the pathological sites.\n",
    "```\n",
    "\n",
    "## Task 3: Using the unlabelled data set in the 'test' folder augment the training data (semi-supervised learning) and report the variation in classification performance on test data set.(15 points)\n",
    "[You may use any method of your choice, one possible way is mentioned below.] \n",
    "\n",
    "```\n",
    "Hint: \n",
    "a. Train a model using the 'train' split.\n",
    "b. Pass the unlabelled images through the trained model and retrieve the dense layer feature prior to classification layer. Using this dense layer as representative of the image, apply label propagation to retrieve labels correspndng to the unbalelled data.\n",
    "c. Next, concatenate the train data with the unlabelled data (that has now been self labelled) and retrain the network.\n",
    "d. Report classification performance on test data\n",
    "Use the unlabelled test data  to improve classification performance by using a semi-supervised label-propagation/self-labelling approach. (20 points)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDtJ-ochsLL9"
   },
   "source": [
    "# Task 1: Create a classifier\n",
    "\n",
    "The first attempt for this task was based on a multi-label strategy. I have kept some of the code for that in the notebook. However, there were several issues with that approach. The biggest one being the system not being able to form a very good decision boundary as a result everything started getting classified into a general category.\n",
    "\n",
    "The second approach is based on a conventional multiclass classifier. However, this also resulted in very low accuracy and unreliable results.\n",
    "\n",
    "The third approach is based binary classification of \n",
    "* diabetic retinopathy vs normal\n",
    "* glaucoma vs normal\n",
    "* other vs normal\n",
    "\n",
    "The reason for this is that all of these ailments look at different parts of retina. So by changing preprocessing we are able to focus at different sections of the scan and make more accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bljPwe0Hac4s"
   },
   "outputs": [],
   "source": [
    "# importing the required modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image, ImageEnhance\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "#from google.colab.patches import cv2_imshow\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.backend import gradients, mean\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "O32oTKmbZJJu"
   },
   "outputs": [],
   "source": [
    "# find path for the data\n",
    "data_path = 'Data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ox7d4ymeaUhY"
   },
   "source": [
    "### Process the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "rn4xxeBraC0r",
    "outputId": "71dab59f-13c7-4c5b-e713-7d9c0b7f3544"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>opacity</th>\n",
       "      <th>diabetic retinopathy</th>\n",
       "      <th>glaucoma</th>\n",
       "      <th>macular edema</th>\n",
       "      <th>macular degeneration</th>\n",
       "      <th>retinal vascular occlusion</th>\n",
       "      <th>normal</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filename</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>c24a1b14d253.jpg</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9ee905a41651.jpg</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3f58d128caf6.jpg</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4ce6599e7b20.jpg</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0def470360e4.jpg</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  opacity  diabetic retinopathy  glaucoma  macular edema  \\\n",
       "filename                                                                   \n",
       "c24a1b14d253.jpg        0                     0         0              0   \n",
       "9ee905a41651.jpg        0                     0         0              0   \n",
       "3f58d128caf6.jpg        0                     0         1              0   \n",
       "4ce6599e7b20.jpg        1                     0         0              0   \n",
       "0def470360e4.jpg        1                     0         0              0   \n",
       "\n",
       "                  macular degeneration  retinal vascular occlusion  normal  \n",
       "filename                                                                    \n",
       "c24a1b14d253.jpg                     0                           1       0  \n",
       "9ee905a41651.jpg                     0                           1       0  \n",
       "3f58d128caf6.jpg                     0                           0       0  \n",
       "4ce6599e7b20.jpg                     1                           0       0  \n",
       "0def470360e4.jpg                     1                           0       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the training labels\n",
    "train_labels = pd.read_csv(os.path.join(data_path, 'train/train.csv'))\n",
    "train_labels.set_index('filename', inplace=True)\n",
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PljE5vNbXwd"
   },
   "source": [
    "### Create Data Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# direcotries containing files\n",
    "normal_dir = 'Data/separated_pics_train/normal'\n",
    "glaucoma_dir = 'Data/separated_pics_train/glaucoma'\n",
    "dbr_dir = 'Data/separated_pics_train/diabetic retinopathy'\n",
    "other_dir = 'Data/separated_pics_train/other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move files to new directories\n",
    "#shutil.copytree(normal_dir, 'Data/norm vs dbr/normal')\n",
    "#shutil.copytree(dbr_dir, 'Data/norm vs dbr/dbr')\n",
    "\n",
    "#shutil.copytree(normal_dir, 'Data/norm vs glc/normal')\n",
    "#shutil.copytree(glaucoma_dir, 'Data/norm vs glc/glc')\n",
    "\n",
    "#shutil.copytree(normal_dir, 'Data/norm vs other/normal')\n",
    "#shutil.copytree(other_dir, 'Data/norm vs other/other')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oR8iRNCIzpvy"
   },
   "source": [
    "### Creating callback for saving models and stopping training if certain accuracy is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HA_7g6MF_K9j"
   },
   "outputs": [],
   "source": [
    "# define callback to stop training if a certain accuracy is achieved\n",
    "class mycallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(seld, epoch, logs={}):\n",
    "        if (logs.get('accuracy')>0.95):\n",
    "            print('\\nReached 95% accuracy so cancelling training')\n",
    "            self.model.stop_training = True\n",
    "    \n",
    "stop_train = mycallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64VmMELl_K9j"
   },
   "outputs": [],
   "source": [
    "# callback for saving checkpoints during training\n",
    "accuracy_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'models/acc_ckpts',\n",
    "    monitor='loss',\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode='auto',\n",
    "    save_freq='epoch',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for lr decay\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch % 10 != 0:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate decay\n",
    "lr_decay = tf.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal vs Diabetic Retinopathy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two category folders\n",
    "norm_dbr = 'Data/norm vs dbr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "c2FOS_XA3Kgb"
   },
   "outputs": [],
   "source": [
    "# define image size and channels\n",
    "IMG_SIZE = 224\n",
    "CHANNELS = 3\n",
    "\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increasing contrast and brightness\n",
    "def contrast(img_np):\n",
    "    img = tf.image.adjust_contrast(img_np, 2)\n",
    "    img = tf.image.adjust_brightness(img, 0.5)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ivs-oNfSwdnW"
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=10.,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='constant',\n",
    "    data_format=\"channels_last\",\n",
    "    preprocessing_function=contrast,\n",
    "    validation_split = 0.3,\n",
    "    rescale=1.0/255.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "nsUJYt0mytWI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 897 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# create training data\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    directory=norm_dbr,\n",
    "    shuffle=True, \n",
    "    target_size=(IMG_SIZE,IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    subset='training'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 383 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# create training data\n",
    "val_gen = train_datagen.flow_from_directory(\n",
    "    directory=norm_dbr,\n",
    "    shuffle=True, \n",
    "    target_size=(IMG_SIZE,IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ev1qOfkS0HYb"
   },
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "#layer 1\n",
    "inputs = tf.keras.Input(shape = (224, 224, 3))\n",
    "x = tf.keras.layers.Conv2D(256, (3, 3), activation='relu')(inputs)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "# shallow feature extraction layer\n",
    "shallow = tf.keras.layers.Conv2D(32, (3,3), activation='relu')(x)\n",
    "shallow = tf.keras.layers.MaxPooling2D((2, 2))(shallow)\n",
    "shallow = tf.keras.layers.Conv2D(32, (3,3), activation='relu')(shallow)\n",
    "shallow = tf.keras.layers.MaxPooling2D((2, 2))(shallow)\n",
    "shallow = tf.keras.layers.Conv2D(32, (3,3), activation='relu')(shallow)\n",
    "shallow = tf.keras.layers.MaxPooling2D((2, 2))(shallow)\n",
    "shallow = tf.keras.layers.Flatten()(shallow)\n",
    "shallow = tf.keras.layers.Dense(64, activation='relu')(shallow)\n",
    "shallow = tf.keras.layers.Dense(64, activation='relu')(shallow)\n",
    "\n",
    "#layer 2\n",
    "x = tf.keras.layers.Conv2D(256, (3, 3), activation='relu')(x)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "#layer 3\n",
    "x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "#layer 4\n",
    "x = tf.keras.layers.Conv2D(128, (1, 1), activation='relu')(x)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "#layer 5\n",
    "x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "# flatten and add nueral layers\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(256, activation='relu')(x) # first dense layer\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x) # second dense layer\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x) # third dense layer\n",
    "x = tf.keras.layers.Dense(64)(x) # fourth dense layer\n",
    "sum_x = tf.keras.layers.add([x , shallow]) # adding shallow layer\n",
    "\n",
    "output = tf.keras.layers.Dense(2, activation='sigmoid')(sum_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "kI5lDdNv2-nu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 222, 222, 256 7168        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 111, 111, 256 0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 109, 109, 256 590080      max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 54, 54, 256)  0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 52, 52, 64)   147520      max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling2D) (None, 26, 26, 64)   0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 26, 26, 128)  8320        max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 109, 109, 32) 73760       max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling2D) (None, 13, 13, 128)  0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2D)  (None, 54, 54, 32)   0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 11, 11, 64)   73792       max_pooling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 52, 52, 32)   9248        max_pooling2d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling2D) (None, 5, 5, 64)     0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling2D) (None, 26, 26, 32)   0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 1600)         0           max_pooling2d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 24, 24, 32)   9248        max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 256)          409856      flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling2D) (None, 12, 12, 32)   0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 128)          32896       dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 4608)         0           max_pooling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 128)          16512       dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 64)           294976      flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 64)           8256        dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 64)           4160        dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 64)           0           dense_12[0][0]                   \n",
      "                                                                 dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 2)            130         add_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 1,685,922\n",
      "Trainable params: 1,685,922\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define & summarize model\n",
    "model = tf.keras.Model(inputs, output)\n",
    "model.summary()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skipping the final activation layer and just getting the logits as these will be used for the final part of the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "14/14 [==============================] - 84s 6s/step - loss: 0.6685 - accuracy: 0.5726 - val_loss: 0.5782 - val_accuracy: 0.7781\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 82s 6s/step - loss: 0.5209 - accuracy: 0.7695 - val_loss: 0.4558 - val_accuracy: 0.8062\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 83s 6s/step - loss: 0.4716 - accuracy: 0.8091 - val_loss: 0.4450 - val_accuracy: 0.8125\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 83s 6s/step - loss: 0.3980 - accuracy: 0.8259 - val_loss: 0.4669 - val_accuracy: 0.8313\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 83s 6s/step - loss: 0.5001 - accuracy: 0.7959 - val_loss: 0.4584 - val_accuracy: 0.8062\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 82s 6s/step - loss: 0.4793 - accuracy: 0.8235 - val_loss: 0.4537 - val_accuracy: 0.8281\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 83s 6s/step - loss: 0.4008 - accuracy: 0.8427 - val_loss: 0.3762 - val_accuracy: 0.8406\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 83s 6s/step - loss: 0.4120 - accuracy: 0.8319 - val_loss: 0.4272 - val_accuracy: 0.8281\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 83s 6s/step - loss: 0.4093 - accuracy: 0.8487 - val_loss: 0.3709 - val_accuracy: 0.8500\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 82s 6s/step - loss: 0.4011 - accuracy: 0.8331 - val_loss: 0.4495 - val_accuracy: 0.8125\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 82s 6s/step - loss: 0.4177 - accuracy: 0.8295 - val_loss: 0.4438 - val_accuracy: 0.8062\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 82s 6s/step - loss: 0.4182 - accuracy: 0.8151 - val_loss: 0.3603 - val_accuracy: 0.8438\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 87s 6s/step - loss: 0.3956 - accuracy: 0.8415 - val_loss: 0.3709 - val_accuracy: 0.8406\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 82s 6s/step - loss: 0.3436 - accuracy: 0.8583 - val_loss: 0.3410 - val_accuracy: 0.8594\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 82s 6s/step - loss: 0.3518 - accuracy: 0.8583 - val_loss: 0.4149 - val_accuracy: 0.8281\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 82s 6s/step - loss: 0.3512 - accuracy: 0.8631 - val_loss: 0.3367 - val_accuracy: 0.8625\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 83s 6s/step - loss: 0.3461 - accuracy: 0.8571 - val_loss: 0.3454 - val_accuracy: 0.8594\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 83s 6s/step - loss: 0.3430 - accuracy: 0.8667 - val_loss: 0.4754 - val_accuracy: 0.7781\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 83s 6s/step - loss: 0.4233 - accuracy: 0.8355 - val_loss: 0.4428 - val_accuracy: 0.7906\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 83s 6s/step - loss: 0.3913 - accuracy: 0.8343 - val_loss: 0.3961 - val_accuracy: 0.8375\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 83s 6s/step - loss: 0.3520 - accuracy: 0.8679 - val_loss: 0.3545 - val_accuracy: 0.8406\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 83s 6s/step - loss: 0.3278 - accuracy: 0.8727 - val_loss: 0.3022 - val_accuracy: 0.8875\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 83s 6s/step - loss: 0.3094 - accuracy: 0.8872 - val_loss: 0.3512 - val_accuracy: 0.8750\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 82s 6s/step - loss: 0.3047 - accuracy: 0.8872 - val_loss: 0.3545 - val_accuracy: 0.8531\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 83s 6s/step - loss: 0.3144 - accuracy: 0.8655 - val_loss: 0.3341 - val_accuracy: 0.8625\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 83s 6s/step - loss: 0.3082 - accuracy: 0.8752 - val_loss: 0.3269 - val_accuracy: 0.8625\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 83s 6s/step - loss: 0.3317 - accuracy: 0.8655 - val_loss: 0.3509 - val_accuracy: 0.8562\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 87s 6s/step - loss: 0.2952 - accuracy: 0.8862 - val_loss: 0.3309 - val_accuracy: 0.8594\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 83s 6s/step - loss: 0.2951 - accuracy: 0.8884 - val_loss: 0.3392 - val_accuracy: 0.8687\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 82s 6s/step - loss: 0.3065 - accuracy: 0.8788 - val_loss: 0.2997 - val_accuracy: 0.8813\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 83s 6s/step - loss: 0.3575 - accuracy: 0.8715 - val_loss: 0.3513 - val_accuracy: 0.8594\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 87s 6s/step - loss: 0.3080 - accuracy: 0.8824 - val_loss: 0.3394 - val_accuracy: 0.8531\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 83s 6s/step - loss: 0.3157 - accuracy: 0.8788 - val_loss: 0.3462 - val_accuracy: 0.8500\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 83s 6s/step - loss: 0.3018 - accuracy: 0.8812 - val_loss: 0.3038 - val_accuracy: 0.8687\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 83s 6s/step - loss: 0.3132 - accuracy: 0.8727 - val_loss: 0.3413 - val_accuracy: 0.8656\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 82s 6s/step - loss: 0.3805 - accuracy: 0.8367 - val_loss: 0.4518 - val_accuracy: 0.8031\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 82s 6s/step - loss: 0.4061 - accuracy: 0.8391 - val_loss: 0.4129 - val_accuracy: 0.8406\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 86s 6s/step - loss: 0.3529 - accuracy: 0.8571 - val_loss: 0.3620 - val_accuracy: 0.8406\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 82s 6s/step - loss: 0.3021 - accuracy: 0.8896 - val_loss: 0.3021 - val_accuracy: 0.8875\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 82s 6s/step - loss: 0.3184 - accuracy: 0.8727 - val_loss: 0.3336 - val_accuracy: 0.8687\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 82s 6s/step - loss: 0.3081 - accuracy: 0.8812 - val_loss: 0.3166 - val_accuracy: 0.8687\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 82s 6s/step - loss: 0.2710 - accuracy: 0.9052 - val_loss: 0.4008 - val_accuracy: 0.8219\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 83s 6s/step - loss: 0.3769 - accuracy: 0.8439 - val_loss: 0.3357 - val_accuracy: 0.8625\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 87s 6s/step - loss: 0.3503 - accuracy: 0.8549 - val_loss: 0.3585 - val_accuracy: 0.8562\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 82s 6s/step - loss: 0.2960 - accuracy: 0.8739 - val_loss: 0.3471 - val_accuracy: 0.8500\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 82s 6s/step - loss: 0.3689 - accuracy: 0.8415 - val_loss: 0.3907 - val_accuracy: 0.8313\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 82s 6s/step - loss: 0.3498 - accuracy: 0.8499 - val_loss: 0.3502 - val_accuracy: 0.8594\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 82s 6s/step - loss: 0.3498 - accuracy: 0.8439 - val_loss: 0.3134 - val_accuracy: 0.8687\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 83s 6s/step - loss: 0.3217 - accuracy: 0.8691 - val_loss: 0.3285 - val_accuracy: 0.8719\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 82s 6s/step - loss: 0.3399 - accuracy: 0.8848 - val_loss: 0.3444 - val_accuracy: 0.8625\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "history = model.fit(train_gen,\n",
    "                    steps_per_epoch=train_gen.samples // BATCH_SIZE,\n",
    "                    epochs= 50,\n",
    "                    verbose=1,\n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_gen.samples // BATCH_SIZE,\n",
    "                    callbacks=[lr_decay])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/dbr_v1.h5')\n",
    "model.save_weights('models/dbr_1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plottig history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([1, 0])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal vs Glaucoma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_glc = 'Data/norm vs glc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "c2FOS_XA3Kgb"
   },
   "outputs": [],
   "source": [
    "# define image size and channels\n",
    "IMG_SIZE = 224\n",
    "CHANNELS = 3\n",
    "\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increasing contrast and brightness\n",
    "def contrast_only(img_np):\n",
    "    img = tf.image.adjust_contrast(img_np, 2)\n",
    "    img = tf.image.adjust_brightness(img, 0.5)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ivs-oNfSwdnW"
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=10.,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='constant',\n",
    "    data_format=\"channels_last\",\n",
    "    preprocessing_function=contrast_only,\n",
    "    validation_split = 0.3,\n",
    "    rescale=1.0/255.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "nsUJYt0mytWI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 897 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# create training data\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    directory=norm_glc,\n",
    "    shuffle=True, \n",
    "    target_size=(IMG_SIZE,IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    subset='training'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 383 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# create training data\n",
    "val_gen = train_datagen.flow_from_directory(\n",
    "    directory=norm_glc,\n",
    "    shuffle=True, \n",
    "    target_size=(IMG_SIZE,IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ev1qOfkS0HYb"
   },
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "#layer 1\n",
    "inputs = tf.keras.Input(shape = (224, 224, 3))\n",
    "x = tf.keras.layers.Conv2D(256, (3, 3), activation='relu')(inputs)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "# shallow feature extraction layer\n",
    "shallow = tf.keras.layers.Conv2D(32, (3,3), activation='relu')(x)\n",
    "shallow = tf.keras.layers.MaxPooling2D((2, 2))(shallow)\n",
    "shallow = tf.keras.layers.Conv2D(32, (3,3), activation='relu')(shallow)\n",
    "shallow = tf.keras.layers.MaxPooling2D((2, 2))(shallow)\n",
    "shallow = tf.keras.layers.Conv2D(32, (3,3), activation='relu')(shallow)\n",
    "shallow = tf.keras.layers.MaxPooling2D((2, 2))(shallow)\n",
    "shallow = tf.keras.layers.Flatten()(shallow)\n",
    "shallow = tf.keras.layers.Dense(64, activation='relu')(shallow)\n",
    "shallow = tf.keras.layers.Dense(64, activation='relu')(shallow)\n",
    "\n",
    "#layer 2\n",
    "x = tf.keras.layers.Conv2D(256, (3, 3), activation='relu')(x)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "#layer 3\n",
    "x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "#layer 4\n",
    "x = tf.keras.layers.Conv2D(128, (1, 1), activation='relu')(x)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "#layer 5\n",
    "x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "# flatten and add nueral layers\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(256, activation='relu')(x) # first dense layer\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x) # second dense layer\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x) # third dense layer\n",
    "x = tf.keras.layers.Dense(64)(x) # fourth dense layer\n",
    "sum_x = tf.keras.layers.add([x , shallow]) # adding shallow layer\n",
    "\n",
    "output = tf.keras.layers.Dense(2, activation='sigmoid')(sum_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kI5lDdNv2-nu"
   },
   "outputs": [],
   "source": [
    "# define & summarize model\n",
    "model = tf.keras.Model(inputs, output)\n",
    "model.summary()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skipping the final activation layer and just getting the logits as these will be used for the final part of the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "history = model.fit(train_gen,\n",
    "                    steps_per_epoch=train_gen.samples // BATCH_SIZE,\n",
    "                    epochs= 50,\n",
    "                    verbose=1,\n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_gen.samples // BATCH_SIZE,)\n",
    "                    #callbacks=[stop_train, accuracy_callback, lr_decay])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/glc_v1.h5')\n",
    "model.save_weights('models/glc_1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plottig history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([1, 0])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal vs Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_oth = 'Data/norm vs other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "c2FOS_XA3Kgb"
   },
   "outputs": [],
   "source": [
    "# define image size and channels\n",
    "IMG_SIZE = 224\n",
    "CHANNELS = 3\n",
    "\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increasing contrast and brightness\n",
    "def contrast(img_np):\n",
    "    img = tf.image.adjust_contrast(img_np, 2)\n",
    "    img = tf.image.adjust_brightness(img, 0.5)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ivs-oNfSwdnW"
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=10.,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='constant',\n",
    "    data_format=\"channels_last\",\n",
    "    preprocessing_function=contrast,\n",
    "    validation_split = 0.3,\n",
    "    rescale=1.0/255.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "nsUJYt0mytWI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 897 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# create training data\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    directory=norm_oth,\n",
    "    shuffle=True, \n",
    "    target_size=(IMG_SIZE,IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    subset='training'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 383 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# create training data\n",
    "val_gen = train_datagen.flow_from_directory(\n",
    "    directory=norm_oth,\n",
    "    shuffle=True, \n",
    "    target_size=(IMG_SIZE,IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ev1qOfkS0HYb"
   },
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "#layer 1\n",
    "inputs = tf.keras.Input(shape = (224, 224, 3))\n",
    "x = tf.keras.layers.Conv2D(256, (3, 3), activation='relu')(inputs)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "# shallow feature extraction layer\n",
    "shallow = tf.keras.layers.Conv2D(32, (3,3), activation='relu')(x)\n",
    "shallow = tf.keras.layers.MaxPooling2D((2, 2))(shallow)\n",
    "shallow = tf.keras.layers.Conv2D(32, (3,3), activation='relu')(shallow)\n",
    "shallow = tf.keras.layers.MaxPooling2D((2, 2))(shallow)\n",
    "shallow = tf.keras.layers.Conv2D(32, (3,3), activation='relu')(shallow)\n",
    "shallow = tf.keras.layers.MaxPooling2D((2, 2))(shallow)\n",
    "shallow = tf.keras.layers.Flatten()(shallow)\n",
    "shallow = tf.keras.layers.Dense(64, activation='relu')(shallow)\n",
    "shallow = tf.keras.layers.Dense(64, activation='relu')(shallow)\n",
    "\n",
    "#layer 2\n",
    "x = tf.keras.layers.Conv2D(256, (3, 3), activation='relu')(x)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "#layer 3\n",
    "x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "#layer 4\n",
    "x = tf.keras.layers.Conv2D(128, (1, 1), activation='relu')(x)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "#layer 5\n",
    "x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "# flatten and add nueral layers\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(256, activation='relu')(x) # first dense layer\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x) # second dense layer\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x) # third dense layer\n",
    "x = tf.keras.layers.Dense(64)(x) # fourth dense layer\n",
    "sum_x = tf.keras.layers.add([x , shallow]) # adding shallow layer\n",
    "\n",
    "output = tf.keras.layers.Dense(2, activation='sigmoid')(sum_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "kI5lDdNv2-nu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 222, 222, 256 7168        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 111, 111, 256 0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 109, 109, 256 590080      max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 54, 54, 256)  0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 52, 52, 64)   147520      max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling2D) (None, 26, 26, 64)   0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 26, 26, 128)  8320        max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 109, 109, 32) 73760       max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling2D) (None, 13, 13, 128)  0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2D)  (None, 54, 54, 32)   0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 11, 11, 64)   73792       max_pooling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 52, 52, 32)   9248        max_pooling2d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling2D) (None, 5, 5, 64)     0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling2D) (None, 26, 26, 32)   0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 1600)         0           max_pooling2d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 24, 24, 32)   9248        max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 256)          409856      flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling2D) (None, 12, 12, 32)   0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 128)          32896       dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 4608)         0           max_pooling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 128)          16512       dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 64)           294976      flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 64)           8256        dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 64)           4160        dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 64)           0           dense_12[0][0]                   \n",
      "                                                                 dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 2)            130         add_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 1,685,922\n",
      "Trainable params: 1,685,922\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define & summarize model\n",
    "model = tf.keras.Model(inputs, output)\n",
    "model.summary()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skipping the final activation layer and just getting the logits as these will be used for the final part of the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "history = model.fit(train_gen,\n",
    "                    steps_per_epoch=train_gen.samples // BATCH_SIZE,\n",
    "                    epochs= 50,\n",
    "                    verbose=1,\n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_gen.samples // BATCH_SIZE,)\n",
    "                    #callbacks=[stop_train, accuracy_callback, lr_decay])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/oth_v1.h5')\n",
    "model.save_weights('models/oth_1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plottig history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([1, 0])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Rana ML_for_Computer_Vision.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
